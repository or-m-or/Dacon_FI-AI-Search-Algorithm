{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab25fd2-6006-4d9a-9d5a-16b6e2c2dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0772b2-9c8c-4bf2-a484-a418eecb96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.2.0 torchvision==0.17.0 python-dotenv faiss-gpu transformers sentence-transformers pandas peft langchain langchain-community langchain-teddynote pymupdf pymupdf4llm accelerate 'autoawq>=0.1.7'  bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc7d50-8afc-48d0-8528-9695807ed7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformers import set_seed\n",
    "\n",
    "# 시드 설정\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    AwqConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n",
    "\n",
    "from langchain.document_loaders import PDFPlumberLoader, PyMuPDFLoader, PyPDFLoader, UnstructuredPDFLoader\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad130c62-5111-4eb6-bc11-d004af0c3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "\n",
    "        # 모델별 설정 딕셔너리\n",
    "        self.model_configs = {\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\":\n",
    "            {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"rtzr/ko-gemma-2-9b-it\": {\n",
    "                \"quantization_config\": BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "                ),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 450\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 선택된 모델\n",
    "        self.llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        self.llm_model_config = self.model_configs[self.llm_model]\n",
    "        self.llm_peft = False\n",
    "        self.llm_peft_checkpoint = \"Meta-Llama-3.1-8B-Instruct-daconfinanace\"\n",
    "        \n",
    "        self.embed_models = [\"intfloat/multilingual-e5-base\", \"jhgan/ko-sbert-nli\", \"intfloat/multilingual-e5-large\"]\n",
    "        self.embed_model = self.embed_models[2]\n",
    "        \n",
    "        self.pdf_loader = \"pymupdf\"\n",
    "        \n",
    "        self.base_directory = \"open/\"\n",
    "        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n",
    "        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n",
    "        self.chunk_size = 512\n",
    "        self.chunk_overlap = 32\n",
    "        \n",
    "        self.ensemble = True\n",
    "        self.bm25_w = 0.5\n",
    "        self.faiss_w = 0.5\n",
    "        \n",
    "        self.is_submit = True\n",
    "        \n",
    "        self.eval_sum_mode = False\n",
    "        \n",
    "        self.output_dir = \"test_results\"\n",
    "        self.output_csv_file = f\"{self.llm_model.split('/')[1]}_{self.embed_model.split('/')[1]}_pdf{self.pdf_loader}_chks{self.chunk_size}_chkovp{self.chunk_overlap}_bm25{self.bm25_w}_faiss{self.faiss_w}_mix_submission.csv\"\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "    \n",
    "        \n",
    "args=Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefcd61-01db-494a-b9f0-d578c88b1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.llm_model)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    \n",
    "    print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.llm_model,\n",
    "        quantization_config=args.llm_model_config['quantization_config'],\n",
    "        torch_dtype=args.llm_model_config['torch_dtype'],\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    if args.llm_peft:\n",
    "        adapter_model = PeftModel.from_pretrained(model, args.llm_peft_checkpoint)\n",
    "    \n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=adapter_model if args.llm_peft else model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=args.llm_model_config['max_token'],\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf49a2d-3b9a-4025-8f1e-b5bdaebfb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834598f-6524-4e8b-9759-4c4071736730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_path(path):\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "    \n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    md_text = pymupdf4llm.to_markdown(file_path)\n",
    "    \n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    md_chunks = md_header_splitter.split_text(md_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(md_chunks)\n",
    "    return splits\n",
    "    \n",
    "def create_vector_db(chunks, model_path, method='faiss'):\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae518c-5533-45d8-be22-5c0f42439bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "\n",
    "        db = create_vector_db(chunks, model_path=args.embed_model)\n",
    "        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
    "        faiss_retriever = db.as_retriever()\n",
    " \n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n",
    "            weights=[args.bm25_w, args.faiss_w],\n",
    "            search_type=\"mmr\",\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "            'db': db,\n",
    "            'retriever': retriever\n",
    "        }\n",
    "        \n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31417cee-0de2-4b4f-8f50-22188a7a1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = args.base_directory  # Your Base Directory\n",
    "train_df = pd.read_csv(args.train_csv_path)\n",
    "test_df = pd.read_csv(args.test_csv_path)\n",
    "train_pdf_databases = None\n",
    "test_pdf_databases = None\n",
    "if args.is_submit:\n",
    "    test_pdf_databases = process_pdfs_from_dataframe(test_df, base_directory)\n",
    "else:\n",
    "    train_pdf_databases = process_pdfs_from_dataframe(train_df, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1e439-9579-4d37-a793-caec3353dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df if args.is_submit else train_df\n",
    "pdf_databases = test_pdf_databases if args.is_submit else train_pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26df7c1-92b1-4880-9f2c-39df87a3c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502d251-d241-4177-b47d-227aecd2a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "\n",
    "    # 정규화된 키로 데이터베이스 검색\n",
    "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "    template = \"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    ### 질문:\n",
    "    {question}\n",
    "    \n",
    "    질문의 핵심만 파악하여 간결하게 1-2문장으로 답변하고, 불필요한 설명은 피하며 요구된 정보만 제공하세요.\n",
    "    \n",
    "    ### 답변:\n",
    "\n",
    "    <|eot_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template) \n",
    "   \n",
    "    # RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 답변 추론\n",
    "    print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd32c0-c107-4ae3-93e1-7679fa588453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(args.output_csv_file, encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab84a4f-57c8-4f4d-8925-4e84c077775e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
