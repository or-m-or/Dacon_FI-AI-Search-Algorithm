{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Meta-Llama-3.1-8B-Instruct 모델 GPTQ 양자화하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting datasets==2.15.0\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.6)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.70.16)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (3.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.24.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (4.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (2024.2.2)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.15.0)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.20.0\n",
      "    Uninstalling datasets-2.20.0:\n",
      "      Successfully uninstalled datasets-2.20.0\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
      "Collecting auto-gptq\n",
      "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.33.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.15.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.26.3)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.2.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.4)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.43.4)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.24.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.7.24)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.10.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Downloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.7.1+cu118 gekko-1.2.1 rouge-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.32-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.30 (from langchain)\n",
      "  Downloading langchain_core-0.2.30-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.3)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.30->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (4.9.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain) (2.4)\n",
      "Downloading langchain-0.2.13-py3-none-any.whl (997 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.30-py3-none-any.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.32-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, pydantic-core, orjson, jsonpatch, greenlet, annotated-types, SQLAlchemy, pydantic, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.32 annotated-types-0.7.0 greenlet-3.0.3 jsonpatch-1.33 langchain-0.2.13 langchain-core-0.2.30 langchain-text-splitters-0.2.2 langsmith-0.1.99 orjson-3.10.7 pydantic-2.8.2 pydantic-core-2.20.1 tenacity-8.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.43.4)\n",
      "Collecting transformers[torch]\n",
      "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.7.24)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.3.101)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.43.4\n",
      "    Uninstalling transformers-4.43.4:\n",
      "      Successfully uninstalled transformers-4.43.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "optimum 1.21.3 requires transformers[sentencepiece]<4.44.0,>=4.29.0, but you have transformers 4.44.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers-4.44.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U peft accelerate optimum\n",
    "!pip install datasets==2.15.0\n",
    "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "!pip install huggingface_hub\n",
    "!pip install langchain\n",
    "!pip install transformers[torch] -U\n",
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA 사용 가능 확인\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 지원되는 데이터셋을 전달하여 모델 양자화하기\n",
    "- 'wikitest2' 데이터셋 사용\n",
    "- 추후 재정정보 데이터셋 전처리 후 사용고려\n",
    "- 4비트 정밀도로 양자화 (지원 : 2,4,6,8)\n",
    "\n",
    "> **전처리 시 고려사항**\n",
    "1. PyPDF2, pdfplumbler, pdfminer 같은 라이브러리로 pdf에서 텍스트 추출\n",
    "2. 불필요한 공백, 페이지 번호, 제목 등 필요하지 않은 정보 제거\n",
    "3. 순서가 맞지 않는 문장 재구성, 정제\n",
    "4. 데이터셋 형태로 저장(JSON, CSV, TXT 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:13:29,834 - INFO - sh2orc/Llama-3.1-Korean-8B-Instruct 모델을 위한 토크나이저 로딩 중...\n",
      "2024-08-13 12:13:30,320 - INFO - 토크나이저 로딩 완료\n",
      "2024-08-13 12:13:30,321 - INFO - sh2orc/Llama-3.1-Korean-8B-Instruct 모델 로딩 및 양자화 설정 적용 중: GPTQConfig(quant_method=<QuantizationMethod.GPTQ: 'gptq'>)\n",
      "2024-08-13 12:13:30,476 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79bc8669add414ab5e70cc00640c1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ead3ec09fa49559c184087c6195202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:14:33,346 - INFO - Start quantizing block model.layers 1/32\n",
      "2024-08-13 12:14:33,347 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:14:35,435 - INFO - Quantizing self_attn.q_proj in block 1/32...\n",
      "2024-08-13 12:14:37,746 - INFO - duration: 2.3090169429779053\n",
      "2024-08-13 12:14:37,749 - INFO - avg loss: 42.028236389160156\n",
      "2024-08-13 12:14:39,831 - INFO - Quantizing self_attn.k_proj in block 1/32...\n",
      "2024-08-13 12:14:42,120 - INFO - duration: 2.286870002746582\n",
      "2024-08-13 12:14:42,123 - INFO - avg loss: 24.211139678955078\n",
      "2024-08-13 12:14:44,212 - INFO - Quantizing self_attn.v_proj in block 1/32...\n",
      "2024-08-13 12:14:46,512 - INFO - duration: 2.2977356910705566\n",
      "2024-08-13 12:14:46,514 - INFO - avg loss: 0.974865198135376\n",
      "2024-08-13 12:14:48,608 - INFO - Quantizing self_attn.o_proj in block 1/32...\n",
      "2024-08-13 12:14:50,941 - INFO - duration: 2.330991268157959\n",
      "2024-08-13 12:14:50,944 - INFO - avg loss: 0.020795434713363647\n",
      "2024-08-13 12:14:53,036 - INFO - Quantizing mlp.gate_proj in block 1/32...\n",
      "2024-08-13 12:14:55,345 - INFO - duration: 2.306201457977295\n",
      "2024-08-13 12:14:55,348 - INFO - avg loss: 20.371206283569336\n",
      "2024-08-13 12:14:57,437 - INFO - Quantizing mlp.up_proj in block 1/32...\n",
      "2024-08-13 12:14:59,786 - INFO - duration: 2.345621109008789\n",
      "2024-08-13 12:14:59,790 - INFO - avg loss: 16.428592681884766\n",
      "2024-08-13 12:15:11,184 - INFO - Quantizing mlp.down_proj in block 1/32...\n",
      "2024-08-13 12:15:20,634 - INFO - duration: 9.448633193969727\n",
      "2024-08-13 12:15:20,636 - INFO - avg loss: 0.07559630274772644\n",
      "2024-08-13 12:15:22,326 - INFO - Start quantizing block model.layers 2/32\n",
      "2024-08-13 12:15:22,327 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:15:24,454 - INFO - Quantizing self_attn.q_proj in block 2/32...\n",
      "2024-08-13 12:15:26,804 - INFO - duration: 2.3476974964141846\n",
      "2024-08-13 12:15:26,806 - INFO - avg loss: 50.9440803527832\n",
      "2024-08-13 12:15:28,919 - INFO - Quantizing self_attn.k_proj in block 2/32...\n",
      "2024-08-13 12:15:31,252 - INFO - duration: 2.330869674682617\n",
      "2024-08-13 12:15:31,254 - INFO - avg loss: 29.199434280395508\n",
      "2024-08-13 12:15:33,374 - INFO - Quantizing self_attn.v_proj in block 2/32...\n",
      "2024-08-13 12:15:35,710 - INFO - duration: 2.3346173763275146\n",
      "2024-08-13 12:15:35,713 - INFO - avg loss: 2.16469407081604\n",
      "2024-08-13 12:15:37,834 - INFO - Quantizing self_attn.o_proj in block 2/32...\n",
      "2024-08-13 12:15:40,195 - INFO - duration: 2.35874605178833\n",
      "2024-08-13 12:15:40,198 - INFO - avg loss: 0.11542366445064545\n",
      "2024-08-13 12:15:42,312 - INFO - Quantizing mlp.gate_proj in block 2/32...\n",
      "2024-08-13 12:15:44,683 - INFO - duration: 2.3697667121887207\n",
      "2024-08-13 12:15:44,687 - INFO - avg loss: 35.947383880615234\n",
      "2024-08-13 12:15:46,802 - INFO - Quantizing mlp.up_proj in block 2/32...\n",
      "2024-08-13 12:15:49,163 - INFO - duration: 2.358579635620117\n",
      "2024-08-13 12:15:49,166 - INFO - avg loss: 30.475196838378906\n",
      "2024-08-13 12:16:00,682 - INFO - Quantizing mlp.down_proj in block 2/32...\n",
      "2024-08-13 12:16:10,103 - INFO - duration: 9.41832423210144\n",
      "2024-08-13 12:16:10,105 - INFO - avg loss: 3.169799566268921\n",
      "2024-08-13 12:16:11,617 - INFO - Start quantizing block model.layers 3/32\n",
      "2024-08-13 12:16:11,619 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:16:13,770 - INFO - Quantizing self_attn.q_proj in block 3/32...\n",
      "2024-08-13 12:16:16,107 - INFO - duration: 2.3349392414093018\n",
      "2024-08-13 12:16:16,110 - INFO - avg loss: 231.9821014404297\n",
      "2024-08-13 12:16:18,242 - INFO - Quantizing self_attn.k_proj in block 3/32...\n",
      "2024-08-13 12:16:20,576 - INFO - duration: 2.3318052291870117\n",
      "2024-08-13 12:16:20,579 - INFO - avg loss: 142.6824188232422\n",
      "2024-08-13 12:16:22,716 - INFO - Quantizing self_attn.v_proj in block 3/32...\n",
      "2024-08-13 12:16:25,045 - INFO - duration: 2.326442003250122\n",
      "2024-08-13 12:16:25,048 - INFO - avg loss: 7.545915603637695\n",
      "2024-08-13 12:16:27,167 - INFO - Quantizing self_attn.o_proj in block 3/32...\n",
      "2024-08-13 12:16:29,510 - INFO - duration: 2.3411972522735596\n",
      "2024-08-13 12:16:29,513 - INFO - avg loss: 0.08965198695659637\n",
      "2024-08-13 12:16:31,640 - INFO - Quantizing mlp.gate_proj in block 3/32...\n",
      "2024-08-13 12:16:33,997 - INFO - duration: 2.354746103286743\n",
      "2024-08-13 12:16:33,999 - INFO - avg loss: 67.15923309326172\n",
      "2024-08-13 12:16:36,124 - INFO - Quantizing mlp.up_proj in block 3/32...\n",
      "2024-08-13 12:16:38,480 - INFO - duration: 2.354539632797241\n",
      "2024-08-13 12:16:38,484 - INFO - avg loss: 53.40184783935547\n",
      "2024-08-13 12:16:50,005 - INFO - Quantizing mlp.down_proj in block 3/32...\n",
      "2024-08-13 12:16:59,431 - INFO - duration: 9.423258066177368\n",
      "2024-08-13 12:16:59,433 - INFO - avg loss: 0.3812776505947113\n",
      "2024-08-13 12:17:00,949 - INFO - Start quantizing block model.layers 4/32\n",
      "2024-08-13 12:17:00,951 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:17:03,113 - INFO - Quantizing self_attn.q_proj in block 4/32...\n",
      "2024-08-13 12:17:05,459 - INFO - duration: 2.343071460723877\n",
      "2024-08-13 12:17:05,461 - INFO - avg loss: 166.1986541748047\n",
      "2024-08-13 12:17:07,605 - INFO - Quantizing self_attn.k_proj in block 4/32...\n",
      "2024-08-13 12:17:09,935 - INFO - duration: 2.3274848461151123\n",
      "2024-08-13 12:17:09,938 - INFO - avg loss: 91.67578125\n",
      "2024-08-13 12:17:12,076 - INFO - Quantizing self_attn.v_proj in block 4/32...\n",
      "2024-08-13 12:17:14,404 - INFO - duration: 2.3262999057769775\n",
      "2024-08-13 12:17:14,407 - INFO - avg loss: 8.517759323120117\n",
      "2024-08-13 12:17:16,526 - INFO - Quantizing self_attn.o_proj in block 4/32...\n",
      "2024-08-13 12:17:18,873 - INFO - duration: 2.3448173999786377\n",
      "2024-08-13 12:17:18,875 - INFO - avg loss: 0.21951833367347717\n",
      "2024-08-13 12:17:20,994 - INFO - Quantizing mlp.gate_proj in block 4/32...\n",
      "2024-08-13 12:17:23,353 - INFO - duration: 2.3560845851898193\n",
      "2024-08-13 12:17:23,356 - INFO - avg loss: 106.84910583496094\n",
      "2024-08-13 12:17:25,480 - INFO - Quantizing mlp.up_proj in block 4/32...\n",
      "2024-08-13 12:17:27,845 - INFO - duration: 2.362523317337036\n",
      "2024-08-13 12:17:27,848 - INFO - avg loss: 74.15166473388672\n",
      "2024-08-13 12:17:39,374 - INFO - Quantizing mlp.down_proj in block 4/32...\n",
      "2024-08-13 12:17:48,791 - INFO - duration: 9.415146350860596\n",
      "2024-08-13 12:17:48,793 - INFO - avg loss: 0.6497781872749329\n",
      "2024-08-13 12:17:50,308 - INFO - Start quantizing block model.layers 5/32\n",
      "2024-08-13 12:17:50,310 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:17:52,470 - INFO - Quantizing self_attn.q_proj in block 5/32...\n",
      "2024-08-13 12:17:54,812 - INFO - duration: 2.339782476425171\n",
      "2024-08-13 12:17:54,814 - INFO - avg loss: 157.26239013671875\n",
      "2024-08-13 12:17:56,958 - INFO - Quantizing self_attn.k_proj in block 5/32...\n",
      "2024-08-13 12:17:59,278 - INFO - duration: 2.317671537399292\n",
      "2024-08-13 12:17:59,280 - INFO - avg loss: 90.3774185180664\n",
      "2024-08-13 12:18:01,423 - INFO - Quantizing self_attn.v_proj in block 5/32...\n",
      "2024-08-13 12:18:03,746 - INFO - duration: 2.3212344646453857\n",
      "2024-08-13 12:18:03,749 - INFO - avg loss: 9.081046104431152\n",
      "2024-08-13 12:18:05,887 - INFO - Quantizing self_attn.o_proj in block 5/32...\n",
      "2024-08-13 12:18:08,241 - INFO - duration: 2.3525967597961426\n",
      "2024-08-13 12:18:08,245 - INFO - avg loss: 0.3407130837440491\n",
      "2024-08-13 12:18:10,374 - INFO - Quantizing mlp.gate_proj in block 5/32...\n",
      "2024-08-13 12:18:12,731 - INFO - duration: 2.35528826713562\n",
      "2024-08-13 12:18:12,735 - INFO - avg loss: 149.834716796875\n",
      "2024-08-13 12:18:14,864 - INFO - Quantizing mlp.up_proj in block 5/32...\n",
      "2024-08-13 12:18:17,208 - INFO - duration: 2.3416824340820312\n",
      "2024-08-13 12:18:17,211 - INFO - avg loss: 92.17945861816406\n",
      "2024-08-13 12:18:28,745 - INFO - Quantizing mlp.down_proj in block 5/32...\n",
      "2024-08-13 12:18:38,132 - INFO - duration: 9.385128021240234\n",
      "2024-08-13 12:18:38,134 - INFO - avg loss: 1.111486554145813\n",
      "2024-08-13 12:18:39,654 - INFO - Start quantizing block model.layers 6/32\n",
      "2024-08-13 12:18:39,656 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:18:41,820 - INFO - Quantizing self_attn.q_proj in block 6/32...\n",
      "2024-08-13 12:18:44,174 - INFO - duration: 2.3526692390441895\n",
      "2024-08-13 12:18:44,177 - INFO - avg loss: 231.38519287109375\n",
      "2024-08-13 12:18:46,320 - INFO - Quantizing self_attn.k_proj in block 6/32...\n",
      "2024-08-13 12:18:48,656 - INFO - duration: 2.333840847015381\n",
      "2024-08-13 12:18:48,659 - INFO - avg loss: 142.35379028320312\n",
      "2024-08-13 12:18:50,803 - INFO - Quantizing self_attn.v_proj in block 6/32...\n",
      "2024-08-13 12:18:53,134 - INFO - duration: 2.3284802436828613\n",
      "2024-08-13 12:18:53,136 - INFO - avg loss: 9.285213470458984\n",
      "2024-08-13 12:18:55,256 - INFO - Quantizing self_attn.o_proj in block 6/32...\n",
      "2024-08-13 12:18:57,594 - INFO - duration: 2.3353450298309326\n",
      "2024-08-13 12:18:57,596 - INFO - avg loss: 0.45885875821113586\n",
      "2024-08-13 12:18:59,721 - INFO - Quantizing mlp.gate_proj in block 6/32...\n",
      "2024-08-13 12:19:02,097 - INFO - duration: 2.374053478240967\n",
      "2024-08-13 12:19:02,099 - INFO - avg loss: 173.08224487304688\n",
      "2024-08-13 12:19:04,228 - INFO - Quantizing mlp.up_proj in block 6/32...\n",
      "2024-08-13 12:19:06,600 - INFO - duration: 2.3690834045410156\n",
      "2024-08-13 12:19:06,602 - INFO - avg loss: 107.45247650146484\n",
      "2024-08-13 12:19:18,139 - INFO - Quantizing mlp.down_proj in block 6/32...\n",
      "2024-08-13 12:19:27,622 - INFO - duration: 9.480671882629395\n",
      "2024-08-13 12:19:27,624 - INFO - avg loss: 1.5954115390777588\n",
      "2024-08-13 12:19:29,139 - INFO - Start quantizing block model.layers 7/32\n",
      "2024-08-13 12:19:29,142 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:19:31,312 - INFO - Quantizing self_attn.q_proj in block 7/32...\n",
      "2024-08-13 12:19:33,648 - INFO - duration: 2.3343465328216553\n",
      "2024-08-13 12:19:33,651 - INFO - avg loss: 193.93023681640625\n",
      "2024-08-13 12:19:35,800 - INFO - Quantizing self_attn.k_proj in block 7/32...\n",
      "2024-08-13 12:19:38,128 - INFO - duration: 2.3265209197998047\n",
      "2024-08-13 12:19:38,131 - INFO - avg loss: 107.45542907714844\n",
      "2024-08-13 12:19:40,278 - INFO - Quantizing self_attn.v_proj in block 7/32...\n",
      "2024-08-13 12:19:42,630 - INFO - duration: 2.3505964279174805\n",
      "2024-08-13 12:19:42,633 - INFO - avg loss: 9.435697555541992\n",
      "2024-08-13 12:19:44,769 - INFO - Quantizing self_attn.o_proj in block 7/32...\n",
      "2024-08-13 12:19:47,196 - INFO - duration: 2.4254062175750732\n",
      "2024-08-13 12:19:47,200 - INFO - avg loss: 0.7809548377990723\n",
      "2024-08-13 12:19:49,322 - INFO - Quantizing mlp.gate_proj in block 7/32...\n",
      "2024-08-13 12:19:51,666 - INFO - duration: 2.3422746658325195\n",
      "2024-08-13 12:19:51,670 - INFO - avg loss: 188.687255859375\n",
      "2024-08-13 12:19:53,793 - INFO - Quantizing mlp.up_proj in block 7/32...\n",
      "2024-08-13 12:19:56,145 - INFO - duration: 2.35026478767395\n",
      "2024-08-13 12:19:56,149 - INFO - avg loss: 115.46710205078125\n",
      "2024-08-13 12:20:07,697 - INFO - Quantizing mlp.down_proj in block 7/32...\n",
      "2024-08-13 12:20:17,139 - INFO - duration: 9.439699649810791\n",
      "2024-08-13 12:20:17,141 - INFO - avg loss: 1.924086332321167\n",
      "2024-08-13 12:20:18,658 - INFO - Start quantizing block model.layers 8/32\n",
      "2024-08-13 12:20:18,661 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:20:20,817 - INFO - Quantizing self_attn.q_proj in block 8/32...\n",
      "2024-08-13 12:20:23,170 - INFO - duration: 2.3501670360565186\n",
      "2024-08-13 12:20:23,173 - INFO - avg loss: 179.76084899902344\n",
      "2024-08-13 12:20:25,326 - INFO - Quantizing self_attn.k_proj in block 8/32...\n",
      "2024-08-13 12:20:27,647 - INFO - duration: 2.318657398223877\n",
      "2024-08-13 12:20:27,650 - INFO - avg loss: 106.5436782836914\n",
      "2024-08-13 12:20:29,799 - INFO - Quantizing self_attn.v_proj in block 8/32...\n",
      "2024-08-13 12:20:32,125 - INFO - duration: 2.3233718872070312\n",
      "2024-08-13 12:20:32,128 - INFO - avg loss: 9.293523788452148\n",
      "2024-08-13 12:20:34,273 - INFO - Quantizing self_attn.o_proj in block 8/32...\n",
      "2024-08-13 12:20:36,597 - INFO - duration: 2.321821928024292\n",
      "2024-08-13 12:20:36,600 - INFO - avg loss: 1.161210060119629\n",
      "2024-08-13 12:20:38,722 - INFO - Quantizing mlp.gate_proj in block 8/32...\n",
      "2024-08-13 12:20:41,091 - INFO - duration: 2.3668031692504883\n",
      "2024-08-13 12:20:41,095 - INFO - avg loss: 190.36679077148438\n",
      "2024-08-13 12:20:43,230 - INFO - Quantizing mlp.up_proj in block 8/32...\n",
      "2024-08-13 12:20:45,624 - INFO - duration: 2.39188289642334\n",
      "2024-08-13 12:20:45,628 - INFO - avg loss: 125.27486419677734\n",
      "2024-08-13 12:20:57,173 - INFO - Quantizing mlp.down_proj in block 8/32...\n",
      "2024-08-13 12:21:06,575 - INFO - duration: 9.400619268417358\n",
      "2024-08-13 12:21:06,578 - INFO - avg loss: 2.2148842811584473\n",
      "2024-08-13 12:21:08,099 - INFO - Start quantizing block model.layers 9/32\n",
      "2024-08-13 12:21:08,101 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:21:10,271 - INFO - Quantizing self_attn.q_proj in block 9/32...\n",
      "2024-08-13 12:21:12,673 - INFO - duration: 2.399696111679077\n",
      "2024-08-13 12:21:12,676 - INFO - avg loss: 236.83633422851562\n",
      "2024-08-13 12:21:14,812 - INFO - Quantizing self_attn.k_proj in block 9/32...\n",
      "2024-08-13 12:21:17,141 - INFO - duration: 2.3270413875579834\n",
      "2024-08-13 12:21:17,144 - INFO - avg loss: 145.73944091796875\n",
      "2024-08-13 12:21:19,294 - INFO - Quantizing self_attn.v_proj in block 9/32...\n",
      "2024-08-13 12:21:21,627 - INFO - duration: 2.331228494644165\n",
      "2024-08-13 12:21:21,630 - INFO - avg loss: 13.035892486572266\n",
      "2024-08-13 12:21:23,766 - INFO - Quantizing self_attn.o_proj in block 9/32...\n",
      "2024-08-13 12:21:26,069 - INFO - duration: 2.3012495040893555\n",
      "2024-08-13 12:21:26,072 - INFO - avg loss: 1.6786541938781738\n",
      "2024-08-13 12:21:28,193 - INFO - Quantizing mlp.gate_proj in block 9/32...\n",
      "2024-08-13 12:21:30,585 - INFO - duration: 2.390007257461548\n",
      "2024-08-13 12:21:30,589 - INFO - avg loss: 203.66647338867188\n",
      "2024-08-13 12:21:32,727 - INFO - Quantizing mlp.up_proj in block 9/32...\n",
      "2024-08-13 12:21:35,103 - INFO - duration: 2.3737270832061768\n",
      "2024-08-13 12:21:35,107 - INFO - avg loss: 131.85604858398438\n",
      "2024-08-13 12:21:46,652 - INFO - Quantizing mlp.down_proj in block 9/32...\n",
      "2024-08-13 12:21:56,051 - INFO - duration: 9.396989107131958\n",
      "2024-08-13 12:21:56,053 - INFO - avg loss: 2.3841235637664795\n",
      "2024-08-13 12:21:57,572 - INFO - Start quantizing block model.layers 10/32\n",
      "2024-08-13 12:21:57,574 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:21:59,724 - INFO - Quantizing self_attn.q_proj in block 10/32...\n",
      "2024-08-13 12:22:02,062 - INFO - duration: 2.335982322692871\n",
      "2024-08-13 12:22:02,065 - INFO - avg loss: 228.21009826660156\n",
      "2024-08-13 12:22:04,214 - INFO - Quantizing self_attn.k_proj in block 10/32...\n",
      "2024-08-13 12:22:06,552 - INFO - duration: 2.3359363079071045\n",
      "2024-08-13 12:22:06,556 - INFO - avg loss: 135.29937744140625\n",
      "2024-08-13 12:22:08,688 - INFO - Quantizing self_attn.v_proj in block 10/32...\n",
      "2024-08-13 12:22:11,021 - INFO - duration: 2.330453872680664\n",
      "2024-08-13 12:22:11,024 - INFO - avg loss: 17.683921813964844\n",
      "2024-08-13 12:22:13,159 - INFO - Quantizing self_attn.o_proj in block 10/32...\n",
      "2024-08-13 12:22:15,513 - INFO - duration: 2.3517191410064697\n",
      "2024-08-13 12:22:15,516 - INFO - avg loss: 1.7565605640411377\n",
      "2024-08-13 12:22:17,645 - INFO - Quantizing mlp.gate_proj in block 10/32...\n",
      "2024-08-13 12:22:20,009 - INFO - duration: 2.3620030879974365\n",
      "2024-08-13 12:22:20,013 - INFO - avg loss: 215.92918395996094\n",
      "2024-08-13 12:22:22,135 - INFO - Quantizing mlp.up_proj in block 10/32...\n",
      "2024-08-13 12:22:24,468 - INFO - duration: 2.3319127559661865\n",
      "2024-08-13 12:22:24,471 - INFO - avg loss: 138.96751403808594\n",
      "2024-08-13 12:22:36,009 - INFO - Quantizing mlp.down_proj in block 10/32...\n",
      "2024-08-13 12:22:45,471 - INFO - duration: 9.460657835006714\n",
      "2024-08-13 12:22:45,473 - INFO - avg loss: 2.6275510787963867\n",
      "2024-08-13 12:22:46,988 - INFO - Start quantizing block model.layers 11/32\n",
      "2024-08-13 12:22:46,990 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:22:49,142 - INFO - Quantizing self_attn.q_proj in block 11/32...\n",
      "2024-08-13 12:22:51,494 - INFO - duration: 2.3497464656829834\n",
      "2024-08-13 12:22:51,496 - INFO - avg loss: 286.0300598144531\n",
      "2024-08-13 12:22:53,643 - INFO - Quantizing self_attn.k_proj in block 11/32...\n",
      "2024-08-13 12:22:55,987 - INFO - duration: 2.3419744968414307\n",
      "2024-08-13 12:22:55,989 - INFO - avg loss: 179.50184631347656\n",
      "2024-08-13 12:22:58,133 - INFO - Quantizing self_attn.v_proj in block 11/32...\n",
      "2024-08-13 12:23:00,463 - INFO - duration: 2.328028917312622\n",
      "2024-08-13 12:23:00,465 - INFO - avg loss: 15.12006950378418\n",
      "2024-08-13 12:23:02,586 - INFO - Quantizing self_attn.o_proj in block 11/32...\n",
      "2024-08-13 12:23:04,934 - INFO - duration: 2.345777750015259\n",
      "2024-08-13 12:23:04,936 - INFO - avg loss: 1.9192898273468018\n",
      "2024-08-13 12:23:07,055 - INFO - Quantizing mlp.gate_proj in block 11/32...\n",
      "2024-08-13 12:23:09,438 - INFO - duration: 2.381199598312378\n",
      "2024-08-13 12:23:09,440 - INFO - avg loss: 208.05307006835938\n",
      "2024-08-13 12:23:11,569 - INFO - Quantizing mlp.up_proj in block 11/32...\n",
      "2024-08-13 12:23:13,941 - INFO - duration: 2.370089292526245\n",
      "2024-08-13 12:23:13,943 - INFO - avg loss: 143.3185272216797\n",
      "2024-08-13 12:23:25,479 - INFO - Quantizing mlp.down_proj in block 11/32...\n",
      "2024-08-13 12:23:34,866 - INFO - duration: 9.385077953338623\n",
      "2024-08-13 12:23:34,868 - INFO - avg loss: 2.797982692718506\n",
      "2024-08-13 12:23:36,385 - INFO - Start quantizing block model.layers 12/32\n",
      "2024-08-13 12:23:36,388 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:23:38,560 - INFO - Quantizing self_attn.q_proj in block 12/32...\n",
      "2024-08-13 12:23:40,907 - INFO - duration: 2.3442063331604004\n",
      "2024-08-13 12:23:40,908 - INFO - avg loss: 237.93594360351562\n",
      "2024-08-13 12:23:43,058 - INFO - Quantizing self_attn.k_proj in block 12/32...\n",
      "2024-08-13 12:23:45,391 - INFO - duration: 2.331113815307617\n",
      "2024-08-13 12:23:45,394 - INFO - avg loss: 152.92620849609375\n",
      "2024-08-13 12:23:47,533 - INFO - Quantizing self_attn.v_proj in block 12/32...\n",
      "2024-08-13 12:23:49,860 - INFO - duration: 2.324761152267456\n",
      "2024-08-13 12:23:49,863 - INFO - avg loss: 14.317408561706543\n",
      "2024-08-13 12:23:51,984 - INFO - Quantizing self_attn.o_proj in block 12/32...\n",
      "2024-08-13 12:23:54,327 - INFO - duration: 2.34072208404541\n",
      "2024-08-13 12:23:54,329 - INFO - avg loss: 2.1274776458740234\n",
      "2024-08-13 12:23:56,448 - INFO - Quantizing mlp.gate_proj in block 12/32...\n",
      "2024-08-13 12:23:58,791 - INFO - duration: 2.3402297496795654\n",
      "2024-08-13 12:23:58,795 - INFO - avg loss: 219.16802978515625\n",
      "2024-08-13 12:24:00,920 - INFO - Quantizing mlp.up_proj in block 12/32...\n",
      "2024-08-13 12:24:03,274 - INFO - duration: 2.3523781299591064\n",
      "2024-08-13 12:24:03,277 - INFO - avg loss: 155.32505798339844\n",
      "2024-08-13 12:24:14,819 - INFO - Quantizing mlp.down_proj in block 12/32...\n",
      "2024-08-13 12:24:24,279 - INFO - duration: 9.458734273910522\n",
      "2024-08-13 12:24:24,281 - INFO - avg loss: 3.108001708984375\n",
      "2024-08-13 12:24:25,801 - INFO - Start quantizing block model.layers 13/32\n",
      "2024-08-13 12:24:25,803 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:24:27,952 - INFO - Quantizing self_attn.q_proj in block 13/32...\n",
      "2024-08-13 12:24:30,326 - INFO - duration: 2.370281457901001\n",
      "2024-08-13 12:24:30,329 - INFO - avg loss: 202.4832763671875\n",
      "2024-08-13 12:24:32,480 - INFO - Quantizing self_attn.k_proj in block 13/32...\n",
      "2024-08-13 12:24:34,807 - INFO - duration: 2.324845314025879\n",
      "2024-08-13 12:24:34,810 - INFO - avg loss: 113.98382568359375\n",
      "2024-08-13 12:24:36,952 - INFO - Quantizing self_attn.v_proj in block 13/32...\n",
      "2024-08-13 12:24:39,273 - INFO - duration: 2.3193159103393555\n",
      "2024-08-13 12:24:39,276 - INFO - avg loss: 16.267288208007812\n",
      "2024-08-13 12:24:41,402 - INFO - Quantizing self_attn.o_proj in block 13/32...\n",
      "2024-08-13 12:24:43,754 - INFO - duration: 2.3502700328826904\n",
      "2024-08-13 12:24:43,757 - INFO - avg loss: 2.5945496559143066\n",
      "2024-08-13 12:24:45,885 - INFO - Quantizing mlp.gate_proj in block 13/32...\n",
      "2024-08-13 12:24:48,256 - INFO - duration: 2.368833303451538\n",
      "2024-08-13 12:24:48,258 - INFO - avg loss: 221.30303955078125\n",
      "2024-08-13 12:24:50,378 - INFO - Quantizing mlp.up_proj in block 13/32...\n",
      "2024-08-13 12:24:52,739 - INFO - duration: 2.359389543533325\n",
      "2024-08-13 12:24:52,741 - INFO - avg loss: 164.78517150878906\n",
      "2024-08-13 12:25:04,282 - INFO - Quantizing mlp.down_proj in block 13/32...\n",
      "2024-08-13 12:25:13,674 - INFO - duration: 9.390153884887695\n",
      "2024-08-13 12:25:13,676 - INFO - avg loss: 3.6053953170776367\n",
      "2024-08-13 12:25:15,193 - INFO - Start quantizing block model.layers 14/32\n",
      "2024-08-13 12:25:15,194 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:25:17,348 - INFO - Quantizing self_attn.q_proj in block 14/32...\n",
      "2024-08-13 12:25:19,683 - INFO - duration: 2.3324007987976074\n",
      "2024-08-13 12:25:19,686 - INFO - avg loss: 278.56903076171875\n",
      "2024-08-13 12:25:21,830 - INFO - Quantizing self_attn.k_proj in block 14/32...\n",
      "2024-08-13 12:25:24,154 - INFO - duration: 2.321295738220215\n",
      "2024-08-13 12:25:24,157 - INFO - avg loss: 184.80929565429688\n",
      "2024-08-13 12:25:26,298 - INFO - Quantizing self_attn.v_proj in block 14/32...\n",
      "2024-08-13 12:25:28,611 - INFO - duration: 2.311035394668579\n",
      "2024-08-13 12:25:28,613 - INFO - avg loss: 19.082439422607422\n",
      "2024-08-13 12:25:30,736 - INFO - Quantizing self_attn.o_proj in block 14/32...\n",
      "2024-08-13 12:25:33,075 - INFO - duration: 2.336509943008423\n",
      "2024-08-13 12:25:33,077 - INFO - avg loss: 3.1353139877319336\n",
      "2024-08-13 12:25:35,207 - INFO - Quantizing mlp.gate_proj in block 14/32...\n",
      "2024-08-13 12:25:37,584 - INFO - duration: 2.3750834465026855\n",
      "2024-08-13 12:25:37,588 - INFO - avg loss: 234.25262451171875\n",
      "2024-08-13 12:25:39,710 - INFO - Quantizing mlp.up_proj in block 14/32...\n",
      "2024-08-13 12:25:42,086 - INFO - duration: 2.374657392501831\n",
      "2024-08-13 12:25:42,089 - INFO - avg loss: 172.9844512939453\n",
      "2024-08-13 12:25:53,628 - INFO - Quantizing mlp.down_proj in block 14/32...\n",
      "2024-08-13 12:26:03,000 - INFO - duration: 9.369945526123047\n",
      "2024-08-13 12:26:03,002 - INFO - avg loss: 3.968719244003296\n",
      "2024-08-13 12:26:04,517 - INFO - Start quantizing block model.layers 15/32\n",
      "2024-08-13 12:26:04,518 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:26:06,679 - INFO - Quantizing self_attn.q_proj in block 15/32...\n",
      "2024-08-13 12:26:08,980 - INFO - duration: 2.2989895343780518\n",
      "2024-08-13 12:26:08,983 - INFO - avg loss: 294.7220458984375\n",
      "2024-08-13 12:26:11,127 - INFO - Quantizing self_attn.k_proj in block 15/32...\n",
      "2024-08-13 12:26:13,412 - INFO - duration: 2.283842086791992\n",
      "2024-08-13 12:26:13,415 - INFO - avg loss: 204.1912841796875\n",
      "2024-08-13 12:26:15,562 - INFO - Quantizing self_attn.v_proj in block 15/32...\n",
      "2024-08-13 12:26:17,909 - INFO - duration: 2.345252752304077\n",
      "2024-08-13 12:26:17,911 - INFO - avg loss: 20.576343536376953\n",
      "2024-08-13 12:26:20,041 - INFO - Quantizing self_attn.o_proj in block 15/32...\n",
      "2024-08-13 12:26:22,370 - INFO - duration: 2.3264260292053223\n",
      "2024-08-13 12:26:22,372 - INFO - avg loss: 3.579751968383789\n",
      "2024-08-13 12:26:24,492 - INFO - Quantizing mlp.gate_proj in block 15/32...\n",
      "2024-08-13 12:26:26,839 - INFO - duration: 2.3456342220306396\n",
      "2024-08-13 12:26:26,841 - INFO - avg loss: 278.03125\n",
      "2024-08-13 12:26:28,976 - INFO - Quantizing mlp.up_proj in block 15/32...\n",
      "2024-08-13 12:26:31,324 - INFO - duration: 2.3460912704467773\n",
      "2024-08-13 12:26:31,327 - INFO - avg loss: 192.51156616210938\n",
      "2024-08-13 12:26:42,863 - INFO - Quantizing mlp.down_proj in block 15/32...\n",
      "2024-08-13 12:26:52,255 - INFO - duration: 9.390642404556274\n",
      "2024-08-13 12:26:52,257 - INFO - avg loss: 4.973325729370117\n",
      "2024-08-13 12:26:53,770 - INFO - Start quantizing block model.layers 16/32\n",
      "2024-08-13 12:26:53,771 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:26:55,929 - INFO - Quantizing self_attn.q_proj in block 16/32...\n",
      "2024-08-13 12:26:58,263 - INFO - duration: 2.331707715988159\n",
      "2024-08-13 12:26:58,265 - INFO - avg loss: 322.79547119140625\n",
      "2024-08-13 12:27:00,417 - INFO - Quantizing self_attn.k_proj in block 16/32...\n",
      "2024-08-13 12:27:02,727 - INFO - duration: 2.3087220191955566\n",
      "2024-08-13 12:27:02,730 - INFO - avg loss: 171.013916015625\n",
      "2024-08-13 12:27:04,866 - INFO - Quantizing self_attn.v_proj in block 16/32...\n",
      "2024-08-13 12:27:07,157 - INFO - duration: 2.2881412506103516\n",
      "2024-08-13 12:27:07,159 - INFO - avg loss: 24.086652755737305\n",
      "2024-08-13 12:27:09,282 - INFO - Quantizing self_attn.o_proj in block 16/32...\n",
      "2024-08-13 12:27:11,610 - INFO - duration: 2.3259952068328857\n",
      "2024-08-13 12:27:11,612 - INFO - avg loss: 3.0098648071289062\n",
      "2024-08-13 12:27:13,732 - INFO - Quantizing mlp.gate_proj in block 16/32...\n",
      "2024-08-13 12:27:16,091 - INFO - duration: 2.3575053215026855\n",
      "2024-08-13 12:27:16,093 - INFO - avg loss: 303.30181884765625\n",
      "2024-08-13 12:27:18,217 - INFO - Quantizing mlp.up_proj in block 16/32...\n",
      "2024-08-13 12:27:20,571 - INFO - duration: 2.3520379066467285\n",
      "2024-08-13 12:27:20,573 - INFO - avg loss: 200.32582092285156\n",
      "2024-08-13 12:27:32,111 - INFO - Quantizing mlp.down_proj in block 16/32...\n",
      "2024-08-13 12:27:41,557 - INFO - duration: 9.443544387817383\n",
      "2024-08-13 12:27:41,558 - INFO - avg loss: 5.929080009460449\n",
      "2024-08-13 12:27:43,073 - INFO - Start quantizing block model.layers 17/32\n",
      "2024-08-13 12:27:43,074 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:27:45,225 - INFO - Quantizing self_attn.q_proj in block 17/32...\n",
      "2024-08-13 12:27:47,563 - INFO - duration: 2.3359267711639404\n",
      "2024-08-13 12:27:47,565 - INFO - avg loss: 310.843017578125\n",
      "2024-08-13 12:27:49,710 - INFO - Quantizing self_attn.k_proj in block 17/32...\n",
      "2024-08-13 12:27:52,021 - INFO - duration: 2.3085434436798096\n",
      "2024-08-13 12:27:52,023 - INFO - avg loss: 180.99252319335938\n",
      "2024-08-13 12:27:54,153 - INFO - Quantizing self_attn.v_proj in block 17/32...\n",
      "2024-08-13 12:27:56,472 - INFO - duration: 2.3167459964752197\n",
      "2024-08-13 12:27:56,475 - INFO - avg loss: 22.79961585998535\n",
      "2024-08-13 12:27:58,598 - INFO - Quantizing self_attn.o_proj in block 17/32...\n",
      "2024-08-13 12:28:00,954 - INFO - duration: 2.3542187213897705\n",
      "2024-08-13 12:28:00,957 - INFO - avg loss: 2.755053758621216\n",
      "2024-08-13 12:28:03,081 - INFO - Quantizing mlp.gate_proj in block 17/32...\n",
      "2024-08-13 12:28:05,465 - INFO - duration: 2.3818747997283936\n",
      "2024-08-13 12:28:05,467 - INFO - avg loss: 326.6124267578125\n",
      "2024-08-13 12:28:07,593 - INFO - Quantizing mlp.up_proj in block 17/32...\n",
      "2024-08-13 12:28:09,976 - INFO - duration: 2.3811159133911133\n",
      "2024-08-13 12:28:09,980 - INFO - avg loss: 206.08139038085938\n",
      "2024-08-13 12:28:21,524 - INFO - Quantizing mlp.down_proj in block 17/32...\n",
      "2024-08-13 12:28:30,945 - INFO - duration: 9.418526887893677\n",
      "2024-08-13 12:28:30,947 - INFO - avg loss: 6.21379280090332\n",
      "2024-08-13 12:28:32,461 - INFO - Start quantizing block model.layers 18/32\n",
      "2024-08-13 12:28:32,463 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:28:34,621 - INFO - Quantizing self_attn.q_proj in block 18/32...\n",
      "2024-08-13 12:28:36,968 - INFO - duration: 2.3453216552734375\n",
      "2024-08-13 12:28:36,971 - INFO - avg loss: 304.5554504394531\n",
      "2024-08-13 12:28:39,111 - INFO - Quantizing self_attn.k_proj in block 18/32...\n",
      "2024-08-13 12:28:41,421 - INFO - duration: 2.307816982269287\n",
      "2024-08-13 12:28:41,424 - INFO - avg loss: 184.3427734375\n",
      "2024-08-13 12:28:43,564 - INFO - Quantizing self_attn.v_proj in block 18/32...\n",
      "2024-08-13 12:28:45,876 - INFO - duration: 2.309361696243286\n",
      "2024-08-13 12:28:45,880 - INFO - avg loss: 23.755367279052734\n",
      "2024-08-13 12:28:48,014 - INFO - Quantizing self_attn.o_proj in block 18/32...\n",
      "2024-08-13 12:28:50,368 - INFO - duration: 2.3512353897094727\n",
      "2024-08-13 12:28:50,371 - INFO - avg loss: 2.197904586791992\n",
      "2024-08-13 12:28:52,505 - INFO - Quantizing mlp.gate_proj in block 18/32...\n",
      "2024-08-13 12:28:54,881 - INFO - duration: 2.373922824859619\n",
      "2024-08-13 12:28:54,885 - INFO - avg loss: 343.88751220703125\n",
      "2024-08-13 12:28:57,011 - INFO - Quantizing mlp.up_proj in block 18/32...\n",
      "2024-08-13 12:28:59,380 - INFO - duration: 2.3674614429473877\n",
      "2024-08-13 12:28:59,385 - INFO - avg loss: 211.97235107421875\n",
      "2024-08-13 12:29:10,923 - INFO - Quantizing mlp.down_proj in block 18/32...\n",
      "2024-08-13 12:29:20,372 - INFO - duration: 9.447044372558594\n",
      "2024-08-13 12:29:20,374 - INFO - avg loss: 7.296626567840576\n",
      "2024-08-13 12:29:21,892 - INFO - Start quantizing block model.layers 19/32\n",
      "2024-08-13 12:29:21,894 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:29:24,047 - INFO - Quantizing self_attn.q_proj in block 19/32...\n",
      "2024-08-13 12:29:26,378 - INFO - duration: 2.3285956382751465\n",
      "2024-08-13 12:29:26,382 - INFO - avg loss: 300.4314880371094\n",
      "2024-08-13 12:29:28,532 - INFO - Quantizing self_attn.k_proj in block 19/32...\n",
      "2024-08-13 12:29:30,862 - INFO - duration: 2.3284213542938232\n",
      "2024-08-13 12:29:30,866 - INFO - avg loss: 195.93148803710938\n",
      "2024-08-13 12:29:33,009 - INFO - Quantizing self_attn.v_proj in block 19/32...\n",
      "2024-08-13 12:29:35,309 - INFO - duration: 2.297760248184204\n",
      "2024-08-13 12:29:35,312 - INFO - avg loss: 23.605676651000977\n",
      "2024-08-13 12:29:37,436 - INFO - Quantizing self_attn.o_proj in block 19/32...\n",
      "2024-08-13 12:29:39,746 - INFO - duration: 2.3075056076049805\n",
      "2024-08-13 12:29:39,749 - INFO - avg loss: 1.412214994430542\n",
      "2024-08-13 12:29:41,878 - INFO - Quantizing mlp.gate_proj in block 19/32...\n",
      "2024-08-13 12:29:44,231 - INFO - duration: 2.3501784801483154\n",
      "2024-08-13 12:29:44,235 - INFO - avg loss: 354.93341064453125\n",
      "2024-08-13 12:29:46,372 - INFO - Quantizing mlp.up_proj in block 19/32...\n",
      "2024-08-13 12:29:48,754 - INFO - duration: 2.3798797130584717\n",
      "2024-08-13 12:29:48,758 - INFO - avg loss: 217.67042541503906\n",
      "2024-08-13 12:30:00,299 - INFO - Quantizing mlp.down_proj in block 19/32...\n",
      "2024-08-13 12:30:09,769 - INFO - duration: 9.468631982803345\n",
      "2024-08-13 12:30:09,771 - INFO - avg loss: 7.065347671508789\n",
      "2024-08-13 12:30:11,287 - INFO - Start quantizing block model.layers 20/32\n",
      "2024-08-13 12:30:11,289 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:30:13,447 - INFO - Quantizing self_attn.q_proj in block 20/32...\n",
      "2024-08-13 12:30:15,810 - INFO - duration: 2.360743284225464\n",
      "2024-08-13 12:30:15,814 - INFO - avg loss: 301.09063720703125\n",
      "2024-08-13 12:30:17,956 - INFO - Quantizing self_attn.k_proj in block 20/32...\n",
      "2024-08-13 12:30:20,253 - INFO - duration: 2.2947027683258057\n",
      "2024-08-13 12:30:20,255 - INFO - avg loss: 176.5435791015625\n",
      "2024-08-13 12:30:22,403 - INFO - Quantizing self_attn.v_proj in block 20/32...\n",
      "2024-08-13 12:30:24,695 - INFO - duration: 2.2898306846618652\n",
      "2024-08-13 12:30:24,698 - INFO - avg loss: 24.614946365356445\n",
      "2024-08-13 12:30:26,821 - INFO - Quantizing self_attn.o_proj in block 20/32...\n",
      "2024-08-13 12:30:29,167 - INFO - duration: 2.343763589859009\n",
      "2024-08-13 12:30:29,170 - INFO - avg loss: 1.3274917602539062\n",
      "2024-08-13 12:30:31,289 - INFO - Quantizing mlp.gate_proj in block 20/32...\n",
      "2024-08-13 12:30:33,652 - INFO - duration: 2.360658884048462\n",
      "2024-08-13 12:30:33,655 - INFO - avg loss: 375.07550048828125\n",
      "2024-08-13 12:30:35,776 - INFO - Quantizing mlp.up_proj in block 20/32...\n",
      "2024-08-13 12:30:38,114 - INFO - duration: 2.3364269733428955\n",
      "2024-08-13 12:30:38,117 - INFO - avg loss: 225.8770294189453\n",
      "2024-08-13 12:30:49,662 - INFO - Quantizing mlp.down_proj in block 20/32...\n",
      "2024-08-13 12:30:59,116 - INFO - duration: 9.45153546333313\n",
      "2024-08-13 12:30:59,118 - INFO - avg loss: 7.475888252258301\n",
      "2024-08-13 12:31:00,632 - INFO - Start quantizing block model.layers 21/32\n",
      "2024-08-13 12:31:00,634 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:31:02,797 - INFO - Quantizing self_attn.q_proj in block 21/32...\n",
      "2024-08-13 12:31:05,176 - INFO - duration: 2.377578020095825\n",
      "2024-08-13 12:31:05,179 - INFO - avg loss: 316.66668701171875\n",
      "2024-08-13 12:31:07,330 - INFO - Quantizing self_attn.k_proj in block 21/32...\n",
      "2024-08-13 12:31:09,661 - INFO - duration: 2.3286612033843994\n",
      "2024-08-13 12:31:09,664 - INFO - avg loss: 199.58489990234375\n",
      "2024-08-13 12:31:11,805 - INFO - Quantizing self_attn.v_proj in block 21/32...\n",
      "2024-08-13 12:31:14,139 - INFO - duration: 2.331873893737793\n",
      "2024-08-13 12:31:14,142 - INFO - avg loss: 28.500093460083008\n",
      "2024-08-13 12:31:16,277 - INFO - Quantizing self_attn.o_proj in block 21/32...\n",
      "2024-08-13 12:31:18,637 - INFO - duration: 2.358736038208008\n",
      "2024-08-13 12:31:18,640 - INFO - avg loss: 1.4085948467254639\n",
      "2024-08-13 12:31:20,759 - INFO - Quantizing mlp.gate_proj in block 21/32...\n",
      "2024-08-13 12:31:23,141 - INFO - duration: 2.379887104034424\n",
      "2024-08-13 12:31:23,144 - INFO - avg loss: 398.2726745605469\n",
      "2024-08-13 12:31:25,270 - INFO - Quantizing mlp.up_proj in block 21/32...\n",
      "2024-08-13 12:31:27,648 - INFO - duration: 2.3764395713806152\n",
      "2024-08-13 12:31:27,651 - INFO - avg loss: 242.45513916015625\n",
      "2024-08-13 12:31:39,200 - INFO - Quantizing mlp.down_proj in block 21/32...\n",
      "2024-08-13 12:31:48,665 - INFO - duration: 9.46352505683899\n",
      "2024-08-13 12:31:48,668 - INFO - avg loss: 8.055961608886719\n",
      "2024-08-13 12:31:50,184 - INFO - Start quantizing block model.layers 22/32\n",
      "2024-08-13 12:31:50,187 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:31:52,347 - INFO - Quantizing self_attn.q_proj in block 22/32...\n",
      "2024-08-13 12:31:54,703 - INFO - duration: 2.3530707359313965\n",
      "2024-08-13 12:31:54,707 - INFO - avg loss: 301.56903076171875\n",
      "2024-08-13 12:31:56,857 - INFO - Quantizing self_attn.k_proj in block 22/32...\n",
      "2024-08-13 12:31:59,171 - INFO - duration: 2.311819553375244\n",
      "2024-08-13 12:31:59,174 - INFO - avg loss: 185.151611328125\n",
      "2024-08-13 12:32:01,322 - INFO - Quantizing self_attn.v_proj in block 22/32...\n",
      "2024-08-13 12:32:03,654 - INFO - duration: 2.3288512229919434\n",
      "2024-08-13 12:32:03,657 - INFO - avg loss: 29.882423400878906\n",
      "2024-08-13 12:32:05,780 - INFO - Quantizing self_attn.o_proj in block 22/32...\n",
      "2024-08-13 12:32:08,131 - INFO - duration: 2.348987340927124\n",
      "2024-08-13 12:32:08,134 - INFO - avg loss: 2.290593147277832\n",
      "2024-08-13 12:32:10,265 - INFO - Quantizing mlp.gate_proj in block 22/32...\n",
      "2024-08-13 12:32:12,633 - INFO - duration: 2.3657114505767822\n",
      "2024-08-13 12:32:12,637 - INFO - avg loss: 426.7432556152344\n",
      "2024-08-13 12:32:14,759 - INFO - Quantizing mlp.up_proj in block 22/32...\n",
      "2024-08-13 12:32:17,121 - INFO - duration: 2.3604342937469482\n",
      "2024-08-13 12:32:17,125 - INFO - avg loss: 257.2078857421875\n",
      "2024-08-13 12:32:28,662 - INFO - Quantizing mlp.down_proj in block 22/32...\n",
      "2024-08-13 12:32:38,132 - INFO - duration: 9.467838287353516\n",
      "2024-08-13 12:32:38,134 - INFO - avg loss: 9.766637802124023\n",
      "2024-08-13 12:32:39,650 - INFO - Start quantizing block model.layers 23/32\n",
      "2024-08-13 12:32:39,652 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:32:41,801 - INFO - Quantizing self_attn.q_proj in block 23/32...\n",
      "2024-08-13 12:32:44,135 - INFO - duration: 2.3319053649902344\n",
      "2024-08-13 12:32:44,138 - INFO - avg loss: 313.28851318359375\n",
      "2024-08-13 12:32:46,291 - INFO - Quantizing self_attn.k_proj in block 23/32...\n",
      "2024-08-13 12:32:48,602 - INFO - duration: 2.309458017349243\n",
      "2024-08-13 12:32:48,605 - INFO - avg loss: 203.236083984375\n",
      "2024-08-13 12:32:50,742 - INFO - Quantizing self_attn.v_proj in block 23/32...\n",
      "2024-08-13 12:32:53,060 - INFO - duration: 2.3160927295684814\n",
      "2024-08-13 12:32:53,062 - INFO - avg loss: 35.67998504638672\n",
      "2024-08-13 12:32:55,191 - INFO - Quantizing self_attn.o_proj in block 23/32...\n",
      "2024-08-13 12:32:57,532 - INFO - duration: 2.338890790939331\n",
      "2024-08-13 12:32:57,535 - INFO - avg loss: 1.8207887411117554\n",
      "2024-08-13 12:32:59,658 - INFO - Quantizing mlp.gate_proj in block 23/32...\n",
      "2024-08-13 12:33:02,016 - INFO - duration: 2.3562369346618652\n",
      "2024-08-13 12:33:02,019 - INFO - avg loss: 437.9147033691406\n",
      "2024-08-13 12:33:04,146 - INFO - Quantizing mlp.up_proj in block 23/32...\n",
      "2024-08-13 12:33:06,522 - INFO - duration: 2.3738951683044434\n",
      "2024-08-13 12:33:06,526 - INFO - avg loss: 267.2826232910156\n",
      "2024-08-13 12:33:18,070 - INFO - Quantizing mlp.down_proj in block 23/32...\n",
      "2024-08-13 12:33:27,494 - INFO - duration: 9.422043561935425\n",
      "2024-08-13 12:33:27,496 - INFO - avg loss: 9.887336730957031\n",
      "2024-08-13 12:33:29,011 - INFO - Start quantizing block model.layers 24/32\n",
      "2024-08-13 12:33:29,013 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:33:31,180 - INFO - Quantizing self_attn.q_proj in block 24/32...\n",
      "2024-08-13 12:33:33,504 - INFO - duration: 2.322471857070923\n",
      "2024-08-13 12:33:33,506 - INFO - avg loss: 311.2845764160156\n",
      "2024-08-13 12:33:35,652 - INFO - Quantizing self_attn.k_proj in block 24/32...\n",
      "2024-08-13 12:33:37,968 - INFO - duration: 2.314829111099243\n",
      "2024-08-13 12:33:37,971 - INFO - avg loss: 197.7870635986328\n",
      "2024-08-13 12:33:40,119 - INFO - Quantizing self_attn.v_proj in block 24/32...\n",
      "2024-08-13 12:33:42,435 - INFO - duration: 2.3134641647338867\n",
      "2024-08-13 12:33:42,437 - INFO - avg loss: 37.16040802001953\n",
      "2024-08-13 12:33:44,588 - INFO - Quantizing self_attn.o_proj in block 24/32...\n",
      "2024-08-13 12:33:46,919 - INFO - duration: 2.3291563987731934\n",
      "2024-08-13 12:33:46,921 - INFO - avg loss: 1.5235705375671387\n",
      "2024-08-13 12:33:49,040 - INFO - Quantizing mlp.gate_proj in block 24/32...\n",
      "2024-08-13 12:33:51,399 - INFO - duration: 2.357175350189209\n",
      "2024-08-13 12:33:51,401 - INFO - avg loss: 459.59844970703125\n",
      "2024-08-13 12:33:53,528 - INFO - Quantizing mlp.up_proj in block 24/32...\n",
      "2024-08-13 12:33:55,901 - INFO - duration: 2.3704237937927246\n",
      "2024-08-13 12:33:55,904 - INFO - avg loss: 282.39813232421875\n",
      "2024-08-13 12:34:07,440 - INFO - Quantizing mlp.down_proj in block 24/32...\n",
      "2024-08-13 12:34:16,860 - INFO - duration: 9.418625354766846\n",
      "2024-08-13 12:34:16,862 - INFO - avg loss: 10.191346168518066\n",
      "2024-08-13 12:34:18,378 - INFO - Start quantizing block model.layers 25/32\n",
      "2024-08-13 12:34:18,380 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:34:20,526 - INFO - Quantizing self_attn.q_proj in block 25/32...\n",
      "2024-08-13 12:34:22,833 - INFO - duration: 2.304760694503784\n",
      "2024-08-13 12:34:22,835 - INFO - avg loss: 313.7362060546875\n",
      "2024-08-13 12:34:24,962 - INFO - Quantizing self_attn.k_proj in block 25/32...\n",
      "2024-08-13 12:34:27,274 - INFO - duration: 2.310629367828369\n",
      "2024-08-13 12:34:27,276 - INFO - avg loss: 193.94436645507812\n",
      "2024-08-13 12:34:29,415 - INFO - Quantizing self_attn.v_proj in block 25/32...\n",
      "2024-08-13 12:34:31,730 - INFO - duration: 2.313189744949341\n",
      "2024-08-13 12:34:31,733 - INFO - avg loss: 48.12168884277344\n",
      "2024-08-13 12:34:33,866 - INFO - Quantizing self_attn.o_proj in block 25/32...\n",
      "2024-08-13 12:34:36,196 - INFO - duration: 2.326904773712158\n",
      "2024-08-13 12:34:36,200 - INFO - avg loss: 1.993638277053833\n",
      "2024-08-13 12:34:38,350 - INFO - Quantizing mlp.gate_proj in block 25/32...\n",
      "2024-08-13 12:34:40,733 - INFO - duration: 2.380890369415283\n",
      "2024-08-13 12:34:40,737 - INFO - avg loss: 490.9942932128906\n",
      "2024-08-13 12:34:42,860 - INFO - Quantizing mlp.up_proj in block 25/32...\n",
      "2024-08-13 12:34:45,245 - INFO - duration: 2.3822648525238037\n",
      "2024-08-13 12:34:45,248 - INFO - avg loss: 301.1416931152344\n",
      "2024-08-13 12:34:56,777 - INFO - Quantizing mlp.down_proj in block 25/32...\n",
      "2024-08-13 12:35:06,224 - INFO - duration: 9.44475507736206\n",
      "2024-08-13 12:35:06,226 - INFO - avg loss: 11.2804536819458\n",
      "2024-08-13 12:35:07,743 - INFO - Start quantizing block model.layers 26/32\n",
      "2024-08-13 12:35:07,745 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:35:09,923 - INFO - Quantizing self_attn.q_proj in block 26/32...\n",
      "2024-08-13 12:35:12,270 - INFO - duration: 2.3452484607696533\n",
      "2024-08-13 12:35:12,274 - INFO - avg loss: 312.1860046386719\n",
      "2024-08-13 12:35:14,414 - INFO - Quantizing self_attn.k_proj in block 26/32...\n",
      "2024-08-13 12:35:16,726 - INFO - duration: 2.30904221534729\n",
      "2024-08-13 12:35:16,728 - INFO - avg loss: 184.77662658691406\n",
      "2024-08-13 12:35:18,869 - INFO - Quantizing self_attn.v_proj in block 26/32...\n",
      "2024-08-13 12:35:21,172 - INFO - duration: 2.3002469539642334\n",
      "2024-08-13 12:35:21,174 - INFO - avg loss: 49.20032501220703\n",
      "2024-08-13 12:35:23,313 - INFO - Quantizing self_attn.o_proj in block 26/32...\n",
      "2024-08-13 12:35:25,641 - INFO - duration: 2.3265018463134766\n",
      "2024-08-13 12:35:25,644 - INFO - avg loss: 2.391836643218994\n",
      "2024-08-13 12:35:27,763 - INFO - Quantizing mlp.gate_proj in block 26/32...\n",
      "2024-08-13 12:35:30,117 - INFO - duration: 2.351135015487671\n",
      "2024-08-13 12:35:30,119 - INFO - avg loss: 526.7301025390625\n",
      "2024-08-13 12:35:32,252 - INFO - Quantizing mlp.up_proj in block 26/32...\n",
      "2024-08-13 12:35:34,603 - INFO - duration: 2.3494060039520264\n",
      "2024-08-13 12:35:34,606 - INFO - avg loss: 323.58367919921875\n",
      "2024-08-13 12:35:46,141 - INFO - Quantizing mlp.down_proj in block 26/32...\n",
      "2024-08-13 12:35:55,634 - INFO - duration: 9.491445541381836\n",
      "2024-08-13 12:35:55,637 - INFO - avg loss: 12.562952041625977\n",
      "2024-08-13 12:35:57,154 - INFO - Start quantizing block model.layers 27/32\n",
      "2024-08-13 12:35:57,156 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:35:59,327 - INFO - Quantizing self_attn.q_proj in block 27/32...\n",
      "2024-08-13 12:36:01,675 - INFO - duration: 2.346006393432617\n",
      "2024-08-13 12:36:01,677 - INFO - avg loss: 310.6707763671875\n",
      "2024-08-13 12:36:03,819 - INFO - Quantizing self_attn.k_proj in block 27/32...\n",
      "2024-08-13 12:36:06,129 - INFO - duration: 2.3086390495300293\n",
      "2024-08-13 12:36:06,132 - INFO - avg loss: 202.22393798828125\n",
      "2024-08-13 12:36:08,286 - INFO - Quantizing self_attn.v_proj in block 27/32...\n",
      "2024-08-13 12:36:10,592 - INFO - duration: 2.3044519424438477\n",
      "2024-08-13 12:36:10,595 - INFO - avg loss: 49.568199157714844\n",
      "2024-08-13 12:36:12,722 - INFO - Quantizing self_attn.o_proj in block 27/32...\n",
      "2024-08-13 12:36:15,073 - INFO - duration: 2.3488504886627197\n",
      "2024-08-13 12:36:15,076 - INFO - avg loss: 3.5938525199890137\n",
      "2024-08-13 12:36:17,204 - INFO - Quantizing mlp.gate_proj in block 27/32...\n",
      "2024-08-13 12:36:19,571 - INFO - duration: 2.365440607070923\n",
      "2024-08-13 12:36:19,574 - INFO - avg loss: 572.3555908203125\n",
      "2024-08-13 12:36:21,700 - INFO - Quantizing mlp.up_proj in block 27/32...\n",
      "2024-08-13 12:36:24,046 - INFO - duration: 2.343655824661255\n",
      "2024-08-13 12:36:24,050 - INFO - avg loss: 350.14508056640625\n",
      "2024-08-13 12:36:35,586 - INFO - Quantizing mlp.down_proj in block 27/32...\n",
      "2024-08-13 12:36:45,047 - INFO - duration: 9.459197044372559\n",
      "2024-08-13 12:36:45,049 - INFO - avg loss: 14.727238655090332\n",
      "2024-08-13 12:36:46,565 - INFO - Start quantizing block model.layers 28/32\n",
      "2024-08-13 12:36:46,567 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:36:48,727 - INFO - Quantizing self_attn.q_proj in block 28/32...\n",
      "2024-08-13 12:36:51,043 - INFO - duration: 2.314298391342163\n",
      "2024-08-13 12:36:51,045 - INFO - avg loss: 318.90228271484375\n",
      "2024-08-13 12:36:53,185 - INFO - Quantizing self_attn.k_proj in block 28/32...\n",
      "2024-08-13 12:36:55,502 - INFO - duration: 2.3144707679748535\n",
      "2024-08-13 12:36:55,504 - INFO - avg loss: 215.25485229492188\n",
      "2024-08-13 12:36:57,653 - INFO - Quantizing self_attn.v_proj in block 28/32...\n",
      "2024-08-13 12:36:59,972 - INFO - duration: 2.3167223930358887\n",
      "2024-08-13 12:36:59,975 - INFO - avg loss: 67.3883056640625\n",
      "2024-08-13 12:37:02,104 - INFO - Quantizing self_attn.o_proj in block 28/32...\n",
      "2024-08-13 12:37:04,441 - INFO - duration: 2.334873676300049\n",
      "2024-08-13 12:37:04,443 - INFO - avg loss: 4.698245048522949\n",
      "2024-08-13 12:37:06,569 - INFO - Quantizing mlp.gate_proj in block 28/32...\n",
      "2024-08-13 12:37:08,936 - INFO - duration: 2.365147113800049\n",
      "2024-08-13 12:37:08,939 - INFO - avg loss: 622.4202880859375\n",
      "2024-08-13 12:37:11,060 - INFO - Quantizing mlp.up_proj in block 28/32...\n",
      "2024-08-13 12:37:13,413 - INFO - duration: 2.35105562210083\n",
      "2024-08-13 12:37:13,416 - INFO - avg loss: 384.43365478515625\n",
      "2024-08-13 12:37:24,956 - INFO - Quantizing mlp.down_proj in block 28/32...\n",
      "2024-08-13 12:37:34,455 - INFO - duration: 9.497124433517456\n",
      "2024-08-13 12:37:34,456 - INFO - avg loss: 18.92953872680664\n",
      "2024-08-13 12:37:35,973 - INFO - Start quantizing block model.layers 29/32\n",
      "2024-08-13 12:37:35,974 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:37:38,141 - INFO - Quantizing self_attn.q_proj in block 29/32...\n",
      "2024-08-13 12:37:40,480 - INFO - duration: 2.3364717960357666\n",
      "2024-08-13 12:37:40,482 - INFO - avg loss: 291.7882995605469\n",
      "2024-08-13 12:37:42,633 - INFO - Quantizing self_attn.k_proj in block 29/32...\n",
      "2024-08-13 12:37:44,959 - INFO - duration: 2.323512077331543\n",
      "2024-08-13 12:37:44,961 - INFO - avg loss: 173.3155517578125\n",
      "2024-08-13 12:37:47,107 - INFO - Quantizing self_attn.v_proj in block 29/32...\n",
      "2024-08-13 12:37:49,430 - INFO - duration: 2.3210716247558594\n",
      "2024-08-13 12:37:49,432 - INFO - avg loss: 61.027339935302734\n",
      "2024-08-13 12:37:51,582 - INFO - Quantizing self_attn.o_proj in block 29/32...\n",
      "2024-08-13 12:37:53,931 - INFO - duration: 2.3472683429718018\n",
      "2024-08-13 12:37:53,934 - INFO - avg loss: 7.215464115142822\n",
      "2024-08-13 12:37:56,066 - INFO - Quantizing mlp.gate_proj in block 29/32...\n",
      "2024-08-13 12:37:58,445 - INFO - duration: 2.3762571811676025\n",
      "2024-08-13 12:37:58,448 - INFO - avg loss: 678.4863891601562\n",
      "2024-08-13 12:38:00,595 - INFO - Quantizing mlp.up_proj in block 29/32...\n",
      "2024-08-13 12:38:02,972 - INFO - duration: 2.374359607696533\n",
      "2024-08-13 12:38:02,974 - INFO - avg loss: 437.1142272949219\n",
      "2024-08-13 12:38:14,511 - INFO - Quantizing mlp.down_proj in block 29/32...\n",
      "2024-08-13 12:38:24,010 - INFO - duration: 9.496768712997437\n",
      "2024-08-13 12:38:24,012 - INFO - avg loss: 24.670303344726562\n",
      "2024-08-13 12:38:25,526 - INFO - Start quantizing block model.layers 30/32\n",
      "2024-08-13 12:38:25,528 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:38:27,683 - INFO - Quantizing self_attn.q_proj in block 30/32...\n",
      "2024-08-13 12:38:30,030 - INFO - duration: 2.345587730407715\n",
      "2024-08-13 12:38:30,033 - INFO - avg loss: 316.3336181640625\n",
      "2024-08-13 12:38:32,169 - INFO - Quantizing self_attn.k_proj in block 30/32...\n",
      "2024-08-13 12:38:34,502 - INFO - duration: 2.3308141231536865\n",
      "2024-08-13 12:38:34,505 - INFO - avg loss: 183.02401733398438\n",
      "2024-08-13 12:38:36,650 - INFO - Quantizing self_attn.v_proj in block 30/32...\n",
      "2024-08-13 12:38:38,980 - INFO - duration: 2.3278539180755615\n",
      "2024-08-13 12:38:38,983 - INFO - avg loss: 71.41181945800781\n",
      "2024-08-13 12:38:41,112 - INFO - Quantizing self_attn.o_proj in block 30/32...\n",
      "2024-08-13 12:38:43,454 - INFO - duration: 2.3405373096466064\n",
      "2024-08-13 12:38:43,456 - INFO - avg loss: 8.477123260498047\n",
      "2024-08-13 12:38:45,580 - INFO - Quantizing mlp.gate_proj in block 30/32...\n",
      "2024-08-13 12:38:47,935 - INFO - duration: 2.3525571823120117\n",
      "2024-08-13 12:38:47,937 - INFO - avg loss: 700.945556640625\n",
      "2024-08-13 12:38:50,066 - INFO - Quantizing mlp.up_proj in block 30/32...\n",
      "2024-08-13 12:38:52,428 - INFO - duration: 2.359999895095825\n",
      "2024-08-13 12:38:52,431 - INFO - avg loss: 471.1976318359375\n",
      "2024-08-13 12:39:03,975 - INFO - Quantizing mlp.down_proj in block 30/32...\n",
      "2024-08-13 12:39:13,372 - INFO - duration: 9.395488500595093\n",
      "2024-08-13 12:39:13,374 - INFO - avg loss: 35.76089859008789\n",
      "2024-08-13 12:39:14,891 - INFO - Start quantizing block model.layers 31/32\n",
      "2024-08-13 12:39:14,893 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:39:17,045 - INFO - Quantizing self_attn.q_proj in block 31/32...\n",
      "2024-08-13 12:39:19,405 - INFO - duration: 2.3572022914886475\n",
      "2024-08-13 12:39:19,408 - INFO - avg loss: 293.40142822265625\n",
      "2024-08-13 12:39:21,555 - INFO - Quantizing self_attn.k_proj in block 31/32...\n",
      "2024-08-13 12:39:23,878 - INFO - duration: 2.3209688663482666\n",
      "2024-08-13 12:39:23,880 - INFO - avg loss: 194.94924926757812\n",
      "2024-08-13 12:39:26,029 - INFO - Quantizing self_attn.v_proj in block 31/32...\n",
      "2024-08-13 12:39:28,347 - INFO - duration: 2.316722869873047\n",
      "2024-08-13 12:39:28,350 - INFO - avg loss: 103.46127319335938\n",
      "2024-08-13 12:39:30,481 - INFO - Quantizing self_attn.o_proj in block 31/32...\n",
      "2024-08-13 12:39:32,830 - INFO - duration: 2.346766948699951\n",
      "2024-08-13 12:39:32,833 - INFO - avg loss: 13.251123428344727\n",
      "2024-08-13 12:39:34,954 - INFO - Quantizing mlp.gate_proj in block 31/32...\n",
      "2024-08-13 12:39:37,371 - INFO - duration: 2.414619207382202\n",
      "2024-08-13 12:39:37,376 - INFO - avg loss: 764.36083984375\n",
      "2024-08-13 12:39:39,498 - INFO - Quantizing mlp.up_proj in block 31/32...\n",
      "2024-08-13 12:39:41,836 - INFO - duration: 2.3362417221069336\n",
      "2024-08-13 12:39:41,839 - INFO - avg loss: 504.56268310546875\n",
      "2024-08-13 12:39:53,384 - INFO - Quantizing mlp.down_proj in block 31/32...\n",
      "2024-08-13 12:40:02,853 - INFO - duration: 9.46589732170105\n",
      "2024-08-13 12:40:02,855 - INFO - avg loss: 71.98995208740234\n",
      "2024-08-13 12:40:04,366 - INFO - Start quantizing block model.layers 32/32\n",
      "2024-08-13 12:40:04,368 - INFO - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 12:40:06,520 - INFO - Quantizing self_attn.q_proj in block 32/32...\n",
      "2024-08-13 12:40:08,871 - INFO - duration: 2.3486011028289795\n",
      "2024-08-13 12:40:08,873 - INFO - avg loss: 232.31521606445312\n",
      "2024-08-13 12:40:11,013 - INFO - Quantizing self_attn.k_proj in block 32/32...\n",
      "2024-08-13 12:40:13,346 - INFO - duration: 2.3308284282684326\n",
      "2024-08-13 12:40:13,349 - INFO - avg loss: 128.8953399658203\n",
      "2024-08-13 12:40:15,496 - INFO - Quantizing self_attn.v_proj in block 32/32...\n",
      "2024-08-13 12:40:17,825 - INFO - duration: 2.326690912246704\n",
      "2024-08-13 12:40:17,827 - INFO - avg loss: 55.518653869628906\n",
      "2024-08-13 12:40:19,955 - INFO - Quantizing self_attn.o_proj in block 32/32...\n",
      "2024-08-13 12:40:22,332 - INFO - duration: 2.375446081161499\n",
      "2024-08-13 12:40:22,334 - INFO - avg loss: 29.380741119384766\n",
      "2024-08-13 12:40:24,459 - INFO - Quantizing mlp.gate_proj in block 32/32...\n",
      "2024-08-13 12:40:26,841 - INFO - duration: 2.379107713699341\n",
      "2024-08-13 12:40:26,843 - INFO - avg loss: 700.190673828125\n",
      "2024-08-13 12:40:28,969 - INFO - Quantizing mlp.up_proj in block 32/32...\n",
      "2024-08-13 12:40:31,340 - INFO - duration: 2.369084119796753\n",
      "2024-08-13 12:40:31,343 - INFO - avg loss: 487.3756408691406\n",
      "2024-08-13 12:40:42,893 - INFO - Quantizing mlp.down_proj in block 32/32...\n",
      "2024-08-13 12:40:52,311 - INFO - duration: 9.414929151535034\n",
      "2024-08-13 12:40:52,313 - INFO - avg loss: 364.849609375\n",
      "2024-08-13 12:40:53,827 - INFO - Packing model...\n",
      "2024-08-13 12:42:26,592 - INFO - model.layers.0.self_attn.k_proj\n",
      "2024-08-13 12:42:27,186 - INFO - model.layers.0.self_attn.o_proj\n",
      "2024-08-13 12:55:29,309 - INFO - model.layers.0.self_attn.q_proj\n",
      "2024-08-13 13:07:56,506 - INFO - model.layers.0.self_attn.v_proj\n",
      "2024-08-13 13:07:57,004 - INFO - model.layers.0.mlp.down_proj\n",
      "2024-08-13 13:52:48,036 - INFO - model.layers.0.mlp.gate_proj\n",
      "2024-08-13 14:05:49,391 - INFO - model.layers.0.mlp.up_proj\n",
      "2024-08-13 14:18:55,763 - INFO - model.layers.1.self_attn.k_proj\n",
      "2024-08-13 14:18:56,250 - INFO - model.layers.1.self_attn.o_proj\n",
      "2024-08-13 14:32:30,211 - INFO - model.layers.1.self_attn.q_proj\n",
      "2024-08-13 14:45:48,109 - INFO - model.layers.1.self_attn.v_proj\n",
      "2024-08-13 14:45:48,648 - INFO - model.layers.1.mlp.down_proj\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 모델 로드 및 양자화\u001b[39;00m\n\u001b[1;32m     25\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 모델 로딩 및 양자화 설정 적용 중: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquantization_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m quant_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# 자동으로 가능한 장치에 할당\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 메모리 절약을 위해 float16 사용\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델 로딩 및 양자화 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4018\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4015\u001b[0m         dispatch_model(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4018\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4019\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[1;32m   4021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/base.py:195\u001b[0m, in \u001b[0;36mHfQuantizer.postprocess_model\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Post-process the model post weights loading.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    Make sure to override the abstract method `_process_model_after_weight_loading`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m            The keyword arguments that are passed along `_process_model_after_weight_loading`.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_model_after_weight_loading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_gptq.py:85\u001b[0m, in \u001b[0;36mGptqHfQuantizer._process_model_after_weight_loading\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mname_or_path\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimum_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m GPTQConfig\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimum_quantizer\u001b[38;5;241m.\u001b[39mto_dict())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/gptq/quantizer.py:569\u001b[0m, in \u001b[0;36mGPTQQuantizer.quantize_model\u001b[0;34m(self, model, tokenizer)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_exllama \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# Step 4: Pack the model at the end (Replacing the layers)\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m model\u001b[38;5;241m.\u001b[39mis_quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    572\u001b[0m model\u001b[38;5;241m.\u001b[39mquantization_method \u001b[38;5;241m=\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/gptq/quantizer.py:648\u001b[0m, in \u001b[0;36mGPTQQuantizer.pack_model\u001b[0;34m(self, model, quantizers)\u001b[0m\n\u001b[1;32m    646\u001b[0m     qlayers[name]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    647\u001b[0m     layers[name], scale, zero, g_idx \u001b[38;5;241m=\u001b[39m layers[name]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), scale\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), zero\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), g_idx\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 648\u001b[0m     \u001b[43mqlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     qlayers[name]\u001b[38;5;241m.\u001b[39mto(layer_device)\n\u001b[1;32m    651\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel packed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py:137\u001b[0m, in \u001b[0;36mQuantLinear.pack\u001b[0;34m(self, linear, scales, zeros, g_idx)\u001b[0m\n\u001b[1;32m    134\u001b[0m intweight \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfeatures):\n\u001b[1;32m    136\u001b[0m     intweight\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 137\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_zeros\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscales\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint)[\n\u001b[1;32m    138\u001b[0m             :, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         ]\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m intweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(intweight, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m intweight \u001b[38;5;241m=\u001b[39m intweight\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "# 로깅 기본 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 모델 ID\n",
    "model_id = \"sh2orc/Llama-3.1-Korean-8B-Instruct\" # 베이스 Llama 3 + 한국어 파인튜닝 모델\n",
    "\n",
    "# GPTQ 양자화 설정\n",
    "quantization_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,       # 일반적으로 사용되는 그룹 크기\n",
    "    dataset=\"wikitext2\",  # 양자화에 사용할 기본 데이터셋, 추후 제공받은 데이터셋을 전처리하여 넣어주는 것도 고려하기\n",
    "    desc_act=False\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "logging.info(f\"{model_id} 모델을 위한 토크나이저 로딩 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "logging.info(\"토크나이저 로딩 완료\")\n",
    "\n",
    "# 모델 로드 및 양자화\n",
    "logging.info(f\"{model_id} 모델 로딩 및 양자화 설정 적용 중: {quantization_config}\")\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",         # 자동으로 가능한 장치에 할당\n",
    "    torch_dtype=torch.float16  # 메모리 절약을 위해 float16 사용\n",
    ")\n",
    "logging.info(\"모델 로딩 및 양자화 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 레이어의 속성을 확인하여 모델이 올바르게 정량화되었는지 확인할 수 있으며, 여기에는 torch.int32 dtype에 있어야 하는 qweight 및 qzeros 속성이 포함되어 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 잘 동작하는지 확인하기 위해 양자화된 모델에 대해 추론을 수행해 보겠습니다. (트랜스포머와 동일한 API를 사용할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "text = \"안녕\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "out = quant_model.generate(**inputs)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "logging.info(f\"생성된 텍스트: {result_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 양자화된 모델 허깅페이스에 업로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Hugging Face Hub 로그인 (최초 1회 실행 필요)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화된 모델을 Hugging Face Hub에 업로드\n",
    "quant_model.push_to_hub(\"ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit\")\n",
    "tokenizer.push_to_hub(\"ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 허깅페이스에 업로드한 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "def setup_llm_pipeline():\n",
    "    # 모델 ID (Hugging Face Hub에서 가져온 양자화된 모델 ID)\n",
    "    model_id = \"ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit\"\n",
    "\n",
    "    # 토크나이저 로드 및 설정\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    # 모델 로드 및 양자화 설정 적용\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"#### [ model ] ####\\n{model}\\n###################\")\n",
    "\n",
    "    # HuggingFacePipeline 객체 생성\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=450,\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이프라인 설정 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 설정\n",
    "hf_pipeline = setup_llm_pipeline()\n",
    "\n",
    "# 텍스트 생성 예시\n",
    "text = \"안녕\"\n",
    "result = hf_pipeline(text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
