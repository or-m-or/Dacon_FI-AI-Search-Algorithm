{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ì¬ì •ì •ë³´ AI ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ê²½ì§„ëŒ€íšŒ - RAG, **\n",
    "> **RAG, **\n",
    "\n",
    "ë³¸ ëŒ€íšŒì˜ ê³¼ì œëŠ” ì¤‘ì•™ì •ë¶€ ì¬ì • ì •ë³´ì— ëŒ€í•œ **ê²€ìƒ‰ ê¸°ëŠ¥**ì„ ê°œì„ í•˜ê³  í™œìš©ë„ë¥¼ ë†’ì´ëŠ” ì§ˆì˜ì‘ë‹µ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. <br>ì´ë¥¼ í†µí•´ ë°©ëŒ€í•œ ì¬ì • ë°ì´í„°ë¥¼ ì¼ë°˜ êµ­ë¯¼ê³¼ ì „ë¬¸ê°€ ëª¨ë‘ê°€ ì‰½ê²Œ ì ‘ê·¼í•˜ê³  í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. <br><br>\n",
    "ë² ì´ìŠ¤ë¼ì¸ì—ì„œëŠ” í‰ê°€ ë°ì´í„°ì…‹ë§Œì„ í™œìš©í•˜ì—¬ source pdf ë§ˆë‹¤ Vector DBë¥¼ êµ¬ì¶•í•œ ë’¤ langchain ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ llama-2-ko-7b ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ RAG í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì¶”ë¡ í•˜ëŠ” ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. <br>( train_setì„ í™œìš©í•œ í›ˆë ¨ ê³¼ì •ì€ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©°, test_set  ì— ëŒ€í•œ ì¶”ë¡ ë§Œ ì§„í–‰í•©ë‹ˆë‹¤. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ğŸ’¡ **NOTE**: ì´ ì˜ˆì œì—ì„œ ì‚¬ìš©í•œ ëª¨ë¸ ë° ë²¡í„° DB \n",
    "\n",
    "1. LLM : mindsignal/rtzr-ko-gemma-2-9b-it-4bit-financesinfo-ver1\n",
    "2. Embed Model : intfloat/multilingual-e5-large\n",
    "3. Vector DB : FAISS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ê°€ì†í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "%pip install accelerate\n",
    "%pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "\n",
    "# Transformer ê¸°ë°˜ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install transformers[torch] -U\n",
    "%pip install datasets\n",
    "\n",
    "# Llama-Index\n",
    "%pip install llama-index\n",
    "%pip install llama-index-llms-huggingface\n",
    "%pip install llama-index-vector-stores-faiss\n",
    "%pip install llama-index-postprocessor-cohere-rerank\n",
    "%pip install llama-index-readers-file pymupdf\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install faiss-cpu\n",
    "\n",
    "# ë°ì´í„°\n",
    "%pip install umap-learn\n",
    "%pip install scikit-learn\n",
    "%pip install tiktoken\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import pymupdf4llm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForCausalLM,\n",
    "    TextStreamer,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Llama-Index\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    StorageContext,\n",
    "    Document, \n",
    "    Settings,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    get_response_synthesizer,\n",
    "    DocumentSummaryIndex,\n",
    "    SummaryIndex\n",
    ")\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "# from llama_index.core.response_synthesizers import TreeSummarize, Refine\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores import FAISSVectorStore\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì´ˆê¸° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì²´ ìƒì„±ì‹œ llm_name, embed_model_name ì¸ê°€\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, llm_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\", embed_model_name: str = \"intfloat/multilingual-e5-base\") -> None:        \n",
    "        \n",
    "        # ëª¨ë¸ë³„ ì„¤ì • ë”•ì…”ë„ˆë¦¬\n",
    "        self.model_configs = {\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\": {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"rtzr/ko-gemma-2-9b-it\": {\n",
    "                \"quantization_config\": self.get_quantization_config(),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 450,\n",
    "            },\n",
    "            \"jjjguz/Llama-3.1-Korean-8B-Instruct-v1\": {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"mindsignal/rtzr-ko-gemma-2-9b-it-4bit-financesinfo-ver1\": {\n",
    "                \"quantization_config\": self.get_quantization_config(),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 450,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        # LLM ëª¨ë¸ ì„¤ì •\n",
    "        self.llm_name = llm_name\n",
    "        self.llm_model_config = self.model_configs[self.llm_name]\n",
    "        self.llm_obj = self.setup_llm_pipeline()\n",
    "        \n",
    "        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "        self.embed_model_name = embed_model_name \n",
    "        self.embed_model_obj = self.setup_embeddings()\n",
    "        \n",
    "        # ë²¡í„° DB ì„¤ì •\n",
    "        self.base_directory = \"open/\"\n",
    "        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n",
    "        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n",
    "        self.chunk_size = 512\n",
    "        self.chunk_overlap = 32\n",
    "                \n",
    "        # ì œì¶œ ë° í‰ê°€ ì„¤ì •\n",
    "        self.is_submit = True\n",
    "        self.eval_sum_mode = False\n",
    "        \n",
    "        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ì„¤ì •\n",
    "        self.output_dir = \"test_results\"\n",
    "        self.output_csv_file = (\n",
    "            f\"{self.llm_name.split('/')[1]}_{self.embed_model_name.split('/')[1]}_\"\n",
    "            f\"pdf_loader_chks{self.chunk_size}_chkovp{self.chunk_overlap}_submission.csv\"\n",
    "        )\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)    \n",
    "    \n",
    "    def get_quantization_config(self):\n",
    "        \"\"\"4-bit ì–‘ìí™” ì„¤ì •ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,  # 4-bit ì–‘ìí™”\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16  # ì—°ì‚°ì— ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…\n",
    "        )\n",
    "        \n",
    "    def setup_embeddings(self):\n",
    "        \"\"\" ì„ë² ë”© ëª¨ë¸ ì„¤ì • \"\"\"\n",
    "        embed_id = self.embed_model_name\n",
    "        \n",
    "        model_kwargs = {'device': 'cuda'}\n",
    "        encode_kwargs = {'normalize_embeddings': True}\n",
    "        embd = HuggingFaceEmbeddings(\n",
    "            model_name=embed_id,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        \n",
    "        store = LocalFileStore(\"./cache/\")\n",
    "        \n",
    "        # Cache Embedding ì‚¬ìš©\n",
    "        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "            underlying_embeddings=embd, \n",
    "            document_embedding_cache=store, \n",
    "            namespace=embed_id\n",
    "        )\n",
    "        \n",
    "        return cached_embeddings\n",
    "        \n",
    "    def setup_llm_pipeline(self):\n",
    "        \"\"\"LLM ì„¤ì • ë° íŒŒì´í”„ë¼ì¸ êµ¬ì„±\"\"\"\n",
    "        model_id = self.llm_name\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì„¤ì •\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.use_default_system_prompt = False\n",
    "        \n",
    "        # ëª¨ë¸ ë¡œë“œ ë° ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=self.llm_model_config[\"quantization_config\"],\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì— í• ë‹¹\n",
    "        accelerator = Accelerator()\n",
    "        model = accelerator.prepare(model)\n",
    "        \n",
    "        print(f\"#### [ model ] ####\\n{model}\\n###################\")\n",
    "        \n",
    "        # ìŠ¤íŠ¸ë¦¬ë¨¸ë¥¼ ì„¤ì •í•˜ì—¬ í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ ì¶œë ¥\n",
    "        streamer = TextStreamer(tokenizer)\n",
    "\n",
    "        # HuggingFacePipeline ê°ì²´ ìƒì„±\n",
    "        text_generation_pipeline = pipeline(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            return_full_text=False,\n",
    "            max_new_tokens=450,\n",
    "            streamer=streamer\n",
    "        )\n",
    "\n",
    "        hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "        return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„ë™ê¸° ì²˜ë¦¬\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë²¡í„° DB êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval/ \n",
    "\n",
    "ì°¸ê³ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ì´ˆê¸°í™”ì‹œ config ê°ì²´, raptor ê°ì²´ ë„£ê¸°\n",
    "class PDFProcessor:\n",
    "    def __init__(self, config, raptor, embed_obj):\n",
    "        self.config = config\n",
    "        self.raptor = raptor\n",
    "        self.embed_obj  = embed_obj\n",
    "        logger.info(\"PDFProcessor ì´ˆê¸°í™” ì™„ë£Œ.\")\n",
    "                \n",
    "    def normalize_path(self, path):\n",
    "        \"\"\" Path ìœ ë‹ˆì½”ë“œ ì •ê·œí™” \"\"\"\n",
    "        normalized_path = unicodedata.normalize('NFD', path)\n",
    "        logger.debug(f\"ì •ê·œí™”ëœ ê²½ë¡œ: {normalized_path}\")\n",
    "        return normalized_path\n",
    "    \n",
    "    def process_pdf(self, file_path) -> List[str]:\n",
    "        \"\"\" PDF íŒŒì¼ ë¡œë“œ, í…ìŠ¤íŠ¸ ì¶”ì¶œ \"\"\"\n",
    "        logger.info(f\"PDF ì²˜ë¦¬ ì¤‘: {file_path}\")\n",
    "        \n",
    "        loader = PyMuPDFReader()\n",
    "        docs0 = loader.load(file_path=Path(file_path))\n",
    "        doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "        docs = [Document(text=doc_text)]\n",
    "        \n",
    "        # docs_md_content = pymupdf4llm.to_markdown(file_path)\n",
    "\n",
    "        # # ë§ˆì¹¨í‘œ ë’¤ ë° \"----\" ì „í›„ì˜ ì¤„ë°”ê¿ˆì„ ì œì™¸í•œ ëª¨ë“  ì¤„ë°”ê¿ˆì„ ì œê±°\n",
    "        # processed_text = re.sub(r'(?<!\\.)(?<!-----)(\\n|\\r\\n)(?!-----)', ' ', docs_md_content)\n",
    "        # logger.debug(\"ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±° ì™„ë£Œ.\")\n",
    "        \n",
    "        # chunk_size_tok = 2000\n",
    "\n",
    "        # text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        #     chunk_size=chunk_size_tok, chunk_overlap=0\n",
    "        # )\n",
    "        # splits = text_splitter.split_text(processed_text)\n",
    "        # logger.info(f\"í…ìŠ¤íŠ¸ë¥¼ {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ.\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def process_pdfs_from_dataframe(self, df):\n",
    "        \"\"\" PDF ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° ë²¡í„° DB êµ¬ì¶• \"\"\"\n",
    "        pdf_databases = {}\n",
    "        unique_paths = df['Source_path'].unique()\n",
    "\n",
    "        for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "            # ê²½ë¡œ ì •ê·œí™” ë° ì ˆëŒ€ ê²½ë¡œ ìƒì„±\n",
    "            normalized_path = self.normalize_path(path)\n",
    "            full_path = os.path.normpath(os.path.join(self.config.base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "\n",
    "            # pdf -> ë²¡í„° DB êµ¬ì¶•\n",
    "            pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "            db_index_path = f\"./RAPTOR_{pdf_title}.faiss\"\n",
    "            \n",
    "            \n",
    "            if os.path.exists(db_index_path):\n",
    "                logging.info(f\"ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì¤‘: {db_index_path}\")\n",
    "                vector_store = FAISS.load_local(db_index_path, embeddings=self.embed_obj, allow_dangerous_deserialization=True)\n",
    "            else:\n",
    "                print(f\"PDF -> ë²¡í„° DB êµ¬ì¶• ì¤‘ : [{pdf_title}]...\")\n",
    "            \n",
    "                leaf_texts = self.process_pdf(full_path) # list[str]\n",
    "\n",
    "                # RAPTOR íŠ¸ë¦¬ êµ¬ì¶•\n",
    "                raptor_results = self.raptor.recursive_embed_cluster_summarize(\n",
    "                    leaf_texts, level=1, n_levels=3\n",
    "                )\n",
    "                logger.info(f\"RAPTOR íŠ¸ë¦¬ êµ¬ì¶• ì™„ë£Œ: {pdf_title}.\")\n",
    "            \n",
    "                # ê° ë ˆë²¨ì˜ ìš”ì•½ì„ ì¶”ì¶œí•˜ì—¬ all_textsì— ì¶”ê°€\n",
    "                all_texts = leaf_texts.copy()\n",
    "                for level in sorted(raptor_results.keys()):\n",
    "                    summaries = raptor_results[level][1][\"summaries\"].tolist()  # í˜„ì¬ ë ˆë²¨ì˜ DataFrameì—ì„œ ìš”ì•½ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "                    all_texts.extend(summaries)                                 # í˜„ì¬ ë ˆë²¨ì˜ ìš”ì•½ì„ all_textsì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "                logger.info(f\"{pdf_title}ì— ëŒ€í•´ ìš”ì•½ ì¶”ê°€ ì™„ë£Œ.\")\n",
    "            \n",
    "                # FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "                vector_store = FAISS.from_texts(texts=all_texts, embedding=self.embed_obj)\n",
    "                vector_store.save_local(db_index_path)\n",
    "                logger.info(f\"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ: {db_index_path}\")\n",
    "            \n",
    "            retriever = vector_store.as_retriever()\n",
    "            \n",
    "            pdf_databases[pdf_title] = {\n",
    "                'db': vector_store,\n",
    "                'retriever' : retriever,\n",
    "                'index_path': db_index_path\n",
    "            }\n",
    "            logger.info(f\"PDF ì²˜ë¦¬ ì™„ë£Œ: {pdf_title}.\")\n",
    "        \n",
    "        logger.info(\"ëª¨ë“  PDF ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "        return pdf_databases\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA ìƒì„± ëª¨ë“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "class QAGenerator:\n",
    "    def __init__(self, config, llm):\n",
    "        self.config = config\n",
    "        self.llm = llm\n",
    "    \n",
    "    def normalize_string(self, s):\n",
    "        \"\"\"ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\"\"\"\n",
    "        return unicodedata.normalize('NFC', s)\n",
    "    \n",
    "    def format_docs(self, docs):\n",
    "        # ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì„ ì´ì–´ë¶™ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def clean_answer(self, answer):\n",
    "        # ë‹¤ì–‘í•œ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ì„ ì œê±°\n",
    "        patterns_to_remove = [\n",
    "            r\"^\\s*###? Response:\\s*\",\n",
    "            r\"^\\s*Response:\\s*\",\n",
    "            r\"^\\s*AI:\\s*\",\n",
    "            r\"^\\s*Quad:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*###? Response:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*Response:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*AI:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*Quad:\\s*\",\n",
    "            r\"<eos>\\s*$\"\n",
    "        ]\n",
    "        for pattern in patterns_to_remove:\n",
    "            answer = re.sub(pattern, \"\", answer)\n",
    "        return answer.strip()\n",
    "        \n",
    "    def generate_answers(self, df, pdf_databases):\n",
    "        \"\"\"DataFrameì˜ ê° ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "            source = self.normalize_string(row['Source'])\n",
    "            query = row['Question']\n",
    "\n",
    "            # ì •ê·œí™”ëœ í‚¤ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰\n",
    "            normalized_keys = {self.normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "            retriever = normalized_keys[source]['retriever']\n",
    "            \n",
    "            # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
    "            system_prompt_template = \"\"\"\n",
    "            ë‹¹ì‹ ì€ ì¤‘ì•™ì •ë¶€ ì¬ì • ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¬ì • ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ëª…í™•í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œì‹œí•˜ì„¸ìš”.\n",
    "            ë‹µë³€ì„ ìƒì„±í•  ë•Œì—ëŠ” \"Response:\", \"AI:\" ë“±ê³¼ ê°™ì€ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ê³ , í•µì‹¬ ì •ë³´ë§Œ ëª…í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "            \n",
    "            ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒ ë‹¨ê³„ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:\n",
    "            1. ì§ˆë¬¸ì„ ì´í•´í•˜ê³  í•„ìš”í•œ ì¬ì • ê°œë…ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "            2. ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ì—¬ ë…¼ë¦¬ì ì¸ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "            3. ê´€ë ¨ ê°œë… ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ë§¥ë½ì„ ëª…í™•íˆ í•©ë‹ˆë‹¤.\n",
    "            \n",
    "            ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë©°, ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ë§Œ íŒŒì•…í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n",
    "            ë‹¨, ì£¼ì–´ì™€ ì„œìˆ ì–´ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì˜¨ì „í•œ ë¬¸ì¥ì„ ì™„ì„±ì‹œì¼œ ìµœëŒ€í•œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "            \n",
    "            ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš” :\n",
    "            {context}            \n",
    "            \"\"\"\n",
    "\n",
    "            # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system_prompt_template),\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            qa_chain = create_stuff_documents_chain(llm=self.llm, prompt=prompt)\n",
    "            rag_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "            \n",
    "            # ë‹µë³€ ì¶”ë¡ \n",
    "            print(f\"Question: {query}\")\n",
    "            result = rag_chain.invoke({\"input\": query})\n",
    "            print(f\"Answer: {result}\\n\")\n",
    "\n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            results.append({\n",
    "                \"Source\": row['Source'],\n",
    "                \"Source_path\": row['Source_path'],\n",
    "                \"Question\": query,\n",
    "                \"Answer\": result\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ìµœì¢… í†µí•©, ì‹¤í–‰ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = Config(\n",
    "        llm_name         = \"mindsignal/rtzr-ko-gemma-2-9b-it-4bit-financesinfo-ver1\", \n",
    "        embed_model_name = \"intfloat/multilingual-e5-large\"\n",
    "    )\n",
    "    \n",
    "    # RaptorClustering ê°ì²´ ì´ˆê¸°í™”\n",
    "    raptor = RaptorClustering(\n",
    "        embed_obj=config.embed_model_obj,\n",
    "        llm_obj=config.llm_obj\n",
    "    )\n",
    "\n",
    "    # PDFProcessor ê°ì²´ ì´ˆê¸°í™” ë° PDF ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° ë²¡í„° DB êµ¬ì¶•\n",
    "    pdf_processor = PDFProcessor(config=config, raptor=raptor, embed_obj=config.embed_model_obj)\n",
    "    \n",
    "    # ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° DBë¥¼ êµ¬ì¶•\n",
    "    test_df = pd.read_csv(config.test_csv_path)\n",
    "    pdf_databases = pdf_processor.process_pdfs_from_dataframe(test_df)\n",
    "    \n",
    "    # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "    qa_generator = QAGenerator(config=config, llm=config.llm_obj)\n",
    "    results = qa_generator.generate_answers(test_df, pdf_databases)\n",
    "\n",
    "    # ì œì¶œ ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ ë° ë‹µë³€ ì¶”ê°€\n",
    "    submit_df = pd.read_csv(f\"./open/sample_submission.csv\")\n",
    "    submit_df['Answer'] = [\n",
    "        qa_generator.clean_answer(item['Answer']['answer']) for item in results\n",
    "    ]\n",
    "    submit_df['Answer'] = submit_df['Answer'].fillna(\"ë°ì´ì½˜\")\n",
    "\n",
    "    # ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    submit_df.to_csv(os.path.join(config.output_dir, config.output_csv_file), encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
