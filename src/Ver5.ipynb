{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **재정정보 AI 검색 알고리즘 경진대회 - RAG, **\n",
    "> **RAG, **\n",
    "\n",
    "본 대회의 과제는 중앙정부 재정 정보에 대한 **검색 기능**을 개선하고 활용도를 높이는 질의응답 알고리즘을 개발하는 것입니다. <br>이를 통해 방대한 재정 데이터를 일반 국민과 전문가 모두가 쉽게 접근하고 활용할 수 있도록 하는 것이 목표입니다. <br><br>\n",
    "베이스라인에서는 평가 데이터셋만을 활용하여 source pdf 마다 Vector DB를 구축한 뒤 langchain 라이브러리와 llama-2-ko-7b 모델을 사용하여 RAG 프로세스를 통해 추론하는 과정을 담고 있습니다. <br>( train_set을 활용한 훈련 과정은 포함하지 않으며, test_set  에 대한 추론만 진행합니다. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "💡 **NOTE**: 이 예제에서 사용한 모델 및 벡터 DB \n",
    "\n",
    "1. LLM : mindsignal/rtzr-ko-gemma-2-9b-it-4bit-financesinfo-ver1\n",
    "2. Embed Model : intfloat/multilingual-e5-large\n",
    "3. Vector DB : FAISS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 가속화 및 메모리 관리\n",
    "%pip install accelerate\n",
    "%pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "\n",
    "# Transformer 기반 모델과 데이터셋 관련 라이브러리\n",
    "%pip install transformers[torch] -U\n",
    "%pip install datasets\n",
    "\n",
    "# Llama-Index\n",
    "%pip install llama-index\n",
    "%pip install llama-index-llms-huggingface\n",
    "%pip install llama-index-vector-stores-faiss\n",
    "%pip install llama-index-postprocessor-cohere-rerank\n",
    "%pip install llama-index-readers-file pymupdf\n",
    "\n",
    "# 벡터 데이터베이스 관련 라이브러리\n",
    "%pip install faiss-cpu\n",
    "\n",
    "# 데이터\n",
    "%pip install umap-learn\n",
    "%pip install scikit-learn\n",
    "%pip install tiktoken\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import pymupdf4llm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForCausalLM,\n",
    "    TextStreamer,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Llama-Index\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    StorageContext,\n",
    "    Document, \n",
    "    Settings,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    get_response_synthesizer,\n",
    "    DocumentSummaryIndex,\n",
    "    SummaryIndex\n",
    ")\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "# from llama_index.core.response_synthesizers import TreeSummarize, Refine\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores import FAISSVectorStore\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 생성시 llm_name, embed_model_name 인가\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, llm_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\", embed_model_name: str = \"intfloat/multilingual-e5-base\") -> None:        \n",
    "        \n",
    "        # 모델별 설정 딕셔너리\n",
    "        self.model_configs = {\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\": {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"rtzr/ko-gemma-2-9b-it\": {\n",
    "                \"quantization_config\": self.get_quantization_config(),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 450,\n",
    "            },\n",
    "            \"jjjguz/Llama-3.1-Korean-8B-Instruct-v1\": {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"mindsignal/rtzr-ko-gemma-2-9b-it-4bit-financesinfo-ver1\": {\n",
    "                \"quantization_config\": self.get_quantization_config(),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 450,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        # LLM 모델 설정\n",
    "        self.llm_name = llm_name\n",
    "        self.llm_model_config = self.model_configs[self.llm_name]\n",
    "        self.llm_obj = self.setup_llm_pipeline()\n",
    "        \n",
    "        # 임베딩 모델 설정\n",
    "        self.embed_model_name = embed_model_name \n",
    "        self.embed_model_obj = self.setup_embeddings()\n",
    "        \n",
    "        # 벡터 DB 설정\n",
    "        self.base_directory = \"open/\"\n",
    "        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n",
    "        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n",
    "        self.chunk_size = 512\n",
    "        self.chunk_overlap = 32\n",
    "                \n",
    "        # 제출 및 평가 설정\n",
    "        self.is_submit = True\n",
    "        self.eval_sum_mode = False\n",
    "        \n",
    "        # 출력 디렉토리 및 파일 설정\n",
    "        self.output_dir = \"test_results\"\n",
    "        self.output_csv_file = (\n",
    "            f\"{self.llm_name.split('/')[1]}_{self.embed_model_name.split('/')[1]}_\"\n",
    "            f\"pdf_loader_chks{self.chunk_size}_chkovp{self.chunk_overlap}_submission.csv\"\n",
    "        )\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)    \n",
    "    \n",
    "    def get_quantization_config(self):\n",
    "        \"\"\"4-bit 양자화 설정을 반환하는 함수\"\"\"\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,  # 4-bit 양자화\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16  # 연산에 사용할 데이터 타입\n",
    "        )\n",
    "        \n",
    "    def setup_embeddings(self):\n",
    "        \"\"\" 임베딩 모델 설정 \"\"\"\n",
    "        embed_id = self.embed_model_name\n",
    "        \n",
    "        model_kwargs = {'device': 'cuda'}\n",
    "        encode_kwargs = {'normalize_embeddings': True}\n",
    "        embd = HuggingFaceEmbeddings(\n",
    "            model_name=embed_id,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        \n",
    "        store = LocalFileStore(\"./cache/\")\n",
    "        \n",
    "        # Cache Embedding 사용\n",
    "        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "            underlying_embeddings=embd, \n",
    "            document_embedding_cache=store, \n",
    "            namespace=embed_id\n",
    "        )\n",
    "        \n",
    "        return cached_embeddings\n",
    "        \n",
    "    def setup_llm_pipeline(self):\n",
    "        \"\"\"LLM 설정 및 파이프라인 구성\"\"\"\n",
    "        model_id = self.llm_name\n",
    "        \n",
    "        # 토크나이저 로드 및 설정\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.use_default_system_prompt = False\n",
    "        \n",
    "        # 모델 로드 및 양자화 설정 적용\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=self.llm_model_config[\"quantization_config\"],\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 모델을 여러 GPU에 할당\n",
    "        accelerator = Accelerator()\n",
    "        model = accelerator.prepare(model)\n",
    "        \n",
    "        print(f\"#### [ model ] ####\\n{model}\\n###################\")\n",
    "        \n",
    "        # 스트리머를 설정하여 토큰이 생성될 때마다 출력\n",
    "        streamer = TextStreamer(tokenizer)\n",
    "\n",
    "        # HuggingFacePipeline 객체 생성\n",
    "        text_generation_pipeline = pipeline(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            return_full_text=False,\n",
    "            max_new_tokens=450,\n",
    "            streamer=streamer\n",
    "        )\n",
    "\n",
    "        hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "        return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비동기 처리\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# GPU 설정\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 전처리 및 벡터 DB 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval/ \n",
    "\n",
    "참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 초기화시 config 객체, raptor 객체 넣기\n",
    "class PDFProcessor:\n",
    "    def __init__(self, config, raptor, embed_obj):\n",
    "        self.config = config\n",
    "        self.raptor = raptor\n",
    "        self.embed_obj  = embed_obj\n",
    "        logger.info(\"PDFProcessor 초기화 완료.\")\n",
    "                \n",
    "    def normalize_path(self, path):\n",
    "        \"\"\" Path 유니코드 정규화 \"\"\"\n",
    "        normalized_path = unicodedata.normalize('NFD', path)\n",
    "        logger.debug(f\"정규화된 경로: {normalized_path}\")\n",
    "        return normalized_path\n",
    "    \n",
    "    def process_pdf(self, file_path) -> List[str]:\n",
    "        \"\"\" PDF 파일 로드, 텍스트 추출 \"\"\"\n",
    "        logger.info(f\"PDF 처리 중: {file_path}\")\n",
    "        \n",
    "        loader = PyMuPDFReader()\n",
    "        docs0 = loader.load(file_path=Path(file_path))\n",
    "        doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "        docs = [Document(text=doc_text)]\n",
    "        \n",
    "        # docs_md_content = pymupdf4llm.to_markdown(file_path)\n",
    "\n",
    "        # # 마침표 뒤 및 \"----\" 전후의 줄바꿈을 제외한 모든 줄바꿈을 제거\n",
    "        # processed_text = re.sub(r'(?<!\\.)(?<!-----)(\\n|\\r\\n)(?!-----)', ' ', docs_md_content)\n",
    "        # logger.debug(\"불필요한 줄바꿈 제거 완료.\")\n",
    "        \n",
    "        # chunk_size_tok = 2000\n",
    "\n",
    "        # text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        #     chunk_size=chunk_size_tok, chunk_overlap=0\n",
    "        # )\n",
    "        # splits = text_splitter.split_text(processed_text)\n",
    "        # logger.info(f\"텍스트를 {len(splits)}개의 청크로 분할 완료.\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def process_pdfs_from_dataframe(self, df):\n",
    "        \"\"\" PDF 데이터셋으로부터 벡터 DB 구축 \"\"\"\n",
    "        pdf_databases = {}\n",
    "        unique_paths = df['Source_path'].unique()\n",
    "\n",
    "        for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "            # 경로 정규화 및 절대 경로 생성\n",
    "            normalized_path = self.normalize_path(path)\n",
    "            full_path = os.path.normpath(os.path.join(self.config.base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "\n",
    "            # pdf -> 벡터 DB 구축\n",
    "            pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "            db_index_path = f\"./RAPTOR_{pdf_title}.faiss\"\n",
    "            \n",
    "            \n",
    "            if os.path.exists(db_index_path):\n",
    "                logging.info(f\"기존 벡터 DB 로드 중: {db_index_path}\")\n",
    "                vector_store = FAISS.load_local(db_index_path, embeddings=self.embed_obj, allow_dangerous_deserialization=True)\n",
    "            else:\n",
    "                print(f\"PDF -> 벡터 DB 구축 중 : [{pdf_title}]...\")\n",
    "            \n",
    "                leaf_texts = self.process_pdf(full_path) # list[str]\n",
    "\n",
    "                # RAPTOR 트리 구축\n",
    "                raptor_results = self.raptor.recursive_embed_cluster_summarize(\n",
    "                    leaf_texts, level=1, n_levels=3\n",
    "                )\n",
    "                logger.info(f\"RAPTOR 트리 구축 완료: {pdf_title}.\")\n",
    "            \n",
    "                # 각 레벨의 요약을 추출하여 all_texts에 추가\n",
    "                all_texts = leaf_texts.copy()\n",
    "                for level in sorted(raptor_results.keys()):\n",
    "                    summaries = raptor_results[level][1][\"summaries\"].tolist()  # 현재 레벨의 DataFrame에서 요약을 추출합니다.\n",
    "                    all_texts.extend(summaries)                                 # 현재 레벨의 요약을 all_texts에 추가합니다.\n",
    "                logger.info(f\"{pdf_title}에 대해 요약 추가 완료.\")\n",
    "            \n",
    "                # FAISS 벡터 데이터베이스 구축\n",
    "                vector_store = FAISS.from_texts(texts=all_texts, embedding=self.embed_obj)\n",
    "                vector_store.save_local(db_index_path)\n",
    "                logger.info(f\"벡터 데이터베이스 저장 경로: {db_index_path}\")\n",
    "            \n",
    "            retriever = vector_store.as_retriever()\n",
    "            \n",
    "            pdf_databases[pdf_title] = {\n",
    "                'db': vector_store,\n",
    "                'retriever' : retriever,\n",
    "                'index_path': db_index_path\n",
    "            }\n",
    "            logger.info(f\"PDF 처리 완료: {pdf_title}.\")\n",
    "        \n",
    "        logger.info(\"모든 PDF 처리 완료.\")\n",
    "        return pdf_databases\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA 생성 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "class QAGenerator:\n",
    "    def __init__(self, config, llm):\n",
    "        self.config = config\n",
    "        self.llm = llm\n",
    "    \n",
    "    def normalize_string(self, s):\n",
    "        \"\"\"유니코드 정규화\"\"\"\n",
    "        return unicodedata.normalize('NFC', s)\n",
    "    \n",
    "    def format_docs(self, docs):\n",
    "        # 문서의 페이지 내용을 이어붙여 반환합니다.\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def clean_answer(self, answer):\n",
    "        # 다양한 불필요한 텍스트 패턴을 제거\n",
    "        patterns_to_remove = [\n",
    "            r\"^\\s*###? Response:\\s*\",\n",
    "            r\"^\\s*Response:\\s*\",\n",
    "            r\"^\\s*AI:\\s*\",\n",
    "            r\"^\\s*Quad:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*###? Response:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*Response:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*AI:\\s*\",\n",
    "            r\"^\\s*\\|\\-\\s*\\n?\\s*\\n?\\s*Quad:\\s*\",\n",
    "            r\"<eos>\\s*$\"\n",
    "        ]\n",
    "        for pattern in patterns_to_remove:\n",
    "            answer = re.sub(pattern, \"\", answer)\n",
    "        return answer.strip()\n",
    "        \n",
    "    def generate_answers(self, df, pdf_databases):\n",
    "        \"\"\"DataFrame의 각 질문에 대해 답변을 생성\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "            source = self.normalize_string(row['Source'])\n",
    "            query = row['Question']\n",
    "\n",
    "            # 정규화된 키로 데이터베이스 검색\n",
    "            normalized_keys = {self.normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "            retriever = normalized_keys[source]['retriever']\n",
    "            \n",
    "            # 프롬프트 엔지니어링\n",
    "            system_prompt_template = \"\"\"\n",
    "            당신은 중앙정부 재정 정책 전문가입니다. 주어진 재정 정보를 바탕으로, 명확하고 정확한 답변을 제시하세요.\n",
    "            답변을 생성할 때에는 \"Response:\", \"AI:\" 등과 같은 불필요한 텍스트를 포함하지 말고, 핵심 정보만 명확하게 전달하세요.\n",
    "            \n",
    "            주어진 질문에 대해 다음 단계에 따라 답변하세요:\n",
    "            1. 질문을 이해하고 필요한 재정 개념을 설명합니다.\n",
    "            2. 질문을 단계별로 분석하여 논리적인 답변을 구성합니다.\n",
    "            3. 관련 개념 간의 관계를 분석하여 맥락을 명확히 합니다.\n",
    "            \n",
    "            답변은 간결하고 명확하게 작성하며, 주어진 질문에 대해 핵심만 파악하여 답변하세요.\n",
    "            단, 주어와 서술어를 적절히 사용하여 온전한 문장을 완성시켜 최대한 자연스럽게 답변해야 합니다.\n",
    "            \n",
    "            다음 정보를 바탕으로 질문에 답하세요 :\n",
    "            {context}            \n",
    "            \"\"\"\n",
    "\n",
    "            # 프롬프트 생성\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system_prompt_template),\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            qa_chain = create_stuff_documents_chain(llm=self.llm, prompt=prompt)\n",
    "            rag_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "            \n",
    "            # 답변 추론\n",
    "            print(f\"Question: {query}\")\n",
    "            result = rag_chain.invoke({\"input\": query})\n",
    "            print(f\"Answer: {result}\\n\")\n",
    "\n",
    "            # 결과 저장\n",
    "            results.append({\n",
    "                \"Source\": row['Source'],\n",
    "                \"Source_path\": row['Source_path'],\n",
    "                \"Question\": query,\n",
    "                \"Answer\": result\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 최종 통합, 실행 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = Config(\n",
    "        llm_name         = \"mindsignal/rtzr-ko-gemma-2-9b-it-4bit-financesinfo-ver1\", \n",
    "        embed_model_name = \"intfloat/multilingual-e5-large\"\n",
    "    )\n",
    "    \n",
    "    # RaptorClustering 객체 초기화\n",
    "    raptor = RaptorClustering(\n",
    "        embed_obj=config.embed_model_obj,\n",
    "        llm_obj=config.llm_obj\n",
    "    )\n",
    "\n",
    "    # PDFProcessor 객체 초기화 및 PDF 데이터셋으로부터 벡터 DB 구축\n",
    "    pdf_processor = PDFProcessor(config=config, raptor=raptor, embed_obj=config.embed_model_obj)\n",
    "    \n",
    "    # 데이터를 로드하고 벡터 DB를 구축\n",
    "    test_df = pd.read_csv(config.test_csv_path)\n",
    "    pdf_databases = pdf_processor.process_pdfs_from_dataframe(test_df)\n",
    "    \n",
    "    # 질문에 대한 답변 생성\n",
    "    qa_generator = QAGenerator(config=config, llm=config.llm_obj)\n",
    "    results = qa_generator.generate_answers(test_df, pdf_databases)\n",
    "\n",
    "    # 제출 샘플 파일 로드 및 답변 추가\n",
    "    submit_df = pd.read_csv(f\"./open/sample_submission.csv\")\n",
    "    submit_df['Answer'] = [\n",
    "        qa_generator.clean_answer(item['Answer']['answer']) for item in results\n",
    "    ]\n",
    "    submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")\n",
    "\n",
    "    # 결과를 CSV 파일로 저장\n",
    "    submit_df.to_csv(os.path.join(config.output_dir, config.output_csv_file), encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
