{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ì¬ì •ì •ë³´ AI ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ê²½ì§„ëŒ€íšŒ - RAG, RAPTOR ì ìš©**\n",
    "> **RAG, RAPTOR ì ìš©, No Finetuning**\n",
    "\n",
    "ë³¸ ëŒ€íšŒì˜ ê³¼ì œëŠ” ì¤‘ì•™ì •ë¶€ ì¬ì • ì •ë³´ì— ëŒ€í•œ **ê²€ìƒ‰ ê¸°ëŠ¥**ì„ ê°œì„ í•˜ê³  í™œìš©ë„ë¥¼ ë†’ì´ëŠ” ì§ˆì˜ì‘ë‹µ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. <br>ì´ë¥¼ í†µí•´ ë°©ëŒ€í•œ ì¬ì • ë°ì´í„°ë¥¼ ì¼ë°˜ êµ­ë¯¼ê³¼ ì „ë¬¸ê°€ ëª¨ë‘ê°€ ì‰½ê²Œ ì ‘ê·¼í•˜ê³  í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. <br><br>\n",
    "ë² ì´ìŠ¤ë¼ì¸ì—ì„œëŠ” í‰ê°€ ë°ì´í„°ì…‹ë§Œì„ í™œìš©í•˜ì—¬ source pdf ë§ˆë‹¤ Vector DBë¥¼ êµ¬ì¶•í•œ ë’¤ langchain ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ llama-2-ko-7b ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ RAG í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì¶”ë¡ í•˜ëŠ” ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. <br>( train_setì„ í™œìš©í•œ í›ˆë ¨ ê³¼ì •ì€ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©°, test_set  ì— ëŒ€í•œ ì¶”ë¡ ë§Œ ì§„í–‰í•©ë‹ˆë‹¤. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ğŸ’¡ **NOTE**: ì´ ì˜ˆì œì—ì„œëŠ” \n",
    "\n",
    "1. LLM : [ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit](https://huggingface.co/sh2orc/Llama-3.1-Korean-8B-Instruct)\n",
    "2. Embed Model : [intfloat/multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ëª¨ë¸ ê°€ì†í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "%pip install accelerate\n",
    "%pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "\n",
    "# Transformer ê¸°ë°˜ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install transformers[torch] -U\n",
    "%pip install datasets\n",
    "\n",
    "# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install langchain\n",
    "%pip install langchainhub\n",
    "%pip install langchain_community\n",
    "%pip install langchain-openai\n",
    "%pip install langchain-anthropic\n",
    "%pip install langchain-huggingface\n",
    "\n",
    "# ë¬¸ì„œ ì²˜ë¦¬ ë° ë¬¸ì„œ ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install PyMuPDF\n",
    "%pip install sentence-transformers\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install faiss-cpu\n",
    "# %pip install faiss-gpu\n",
    "%pip install chromadb\n",
    "\n",
    "# ë°ì´í„° ì‹œê°í™” ë° ë¶„ì„ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "%pip install umap-learn\n",
    "%pip install scikit-learn\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import unicodedata\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "import umap\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForCausalLM,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RAPTOR íŠ¸ë¦¬ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. UMAP ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê¸€ë¡œë²Œ ì„ë² ë”© ì°¨ì› ì¶•ì†Œ í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "- ì „ì—­ í´ëŸ¬ìŠ¤í„°ë§ì€ ë°ì´í„° ë‚´ì˜ ì „ë°˜ì ì¸ ì „ì²´ íŒ¨í„´ì„ ì´í•´í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤. <br>\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `global_cluster_embeddings(embeddings, dim, n_neighbors, metric) -> np.ndarray`\n",
    "\n",
    "<br>\n",
    "\n",
    "> **UMAP (Univorm Manifold Approximation and Projection)**\n",
    "- ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” ë¹„ì„ í˜• ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "- ì£¼ë¡œ ì‹œê°í™”ì™€ í´ëŸ¬ìŠ¤í„°ë§ ì§€ì›í•˜ëŠ”ë° ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- UMAPì€ ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ ì‚¬ì„±ì„ ìœ ì§€í•˜ë©°, ë°ì´í„°ë¥¼ ë‚®ì€ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ì¤‘ìš”í•œ íŒ¨í„´ì„ ì‰½ê²Œ ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2024  # ì¬í˜„ì„±ì„ ìœ„í•œ ê³ ì •ëœ ì‹œë“œ ê°’\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    UMAPì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì˜ ì „ì—­ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - embeddings  : numpy ë°°ì—´ë¡œ ëœ ì…ë ¥ ì„ë² ë”©.\n",
    "    - dim         : ì¶•ì†Œí•  ì°¨ì› ê°’.\n",
    "    - n_neighbors : ì„ íƒ ì‚¬í•­; ê° ì ì„ ê³ ë ¤í•  ì´ì›ƒì˜ ìˆ˜. (ê¸°ë³¸ê°’ : ì„ë² ë”© ìˆ˜ì˜ ì œê³±ê·¼)\n",
    "    - metric      : UMAPì— ì‚¬ìš©í•  ê±°ë¦¬ ì¸¡ì • ê¸°ì¤€.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - ì§€ì •ëœ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ì„ë² ë”©ì˜ numpy ë°°ì—´.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. UMAP ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ ì„ë² ë”© ì°¨ì› ì¶•ì†Œ í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "- ì§€ì—­ ì°¨ì› ì¶•ì†ŒëŠ” íŠ¹ì • ë°ì´í„° í¬ì¸íŠ¸ì™€ ê·¸ ì£¼ë³€ ë°ì´í„°ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ë” ì˜ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `local_cluster_embeddings(embeddings, dim, num_neighbors, metric) -> np.ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ì„ë² ë”©ì— ëŒ€í•´ ì§€ì—­ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì „ì—­ í´ëŸ¬ìŠ¤í„°ë§ ì´í›„ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - embeddings    : numpy ë°°ì—´ë¡œì„œì˜ ì…ë ¥ ì„ë² ë”©.\n",
    "    - dim           : ì¶•ì†Œí•  ì°¨ì› ê°’.\n",
    "    - num_neighbors : ê° ì ì— ëŒ€í•´ ê³ ë ¤í•  ì´ì›ƒì˜ ìˆ˜.\n",
    "    - metric        : UMAPì— ì‚¬ìš©í•  ê±°ë¦¬ ì¸¡ì • ê¸°ì¤€.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - ì§€ì •ëœ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ì„ë² ë”©ì˜ numpy ë°°ì—´.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. GMMê³¼ BICë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "- ì£¼ì–´ì§„ ì„ë² ë”© ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•˜ëŠ”ë° ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ì¦‰, í´ëŸ¬ìŠ¤í„°ë§ ë¬¸ì œì—ì„œ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- **ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(Gaussian Mixture Model, GMM)** ê³¼ **ë² ì´ì§€ì•ˆ ì •ë³´ ê¸°ì¤€(BIC)** ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `get_optimal_clusters(embeddings, max_clusters, random_state) -> int`\n",
    "\n",
    "<br>\n",
    "\n",
    "> **GMM (Gaussian Mixture Model, ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸)**\n",
    "- ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„°ì— ê±¸ì³ ë°ì´í„° í¬ì¸íŠ¸ì˜ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤.\n",
    "- ëª¨ë¸ì˜ ë² ì´ì§€ì•ˆ ì •ë³´ ê¸°ì¤€(BIC)ë¥¼ í‰ê°€í•˜ì—¬ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "> **BIC (Bayesian Information Criterion, ë² ì´ì§€ì•ˆ ì •ë³´ ê¸°ì¤€)**\n",
    "- ëª¨ë¸ì˜ ì í•©ë„ì™€ ë³µì¡ë„ ê°„ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ì§€í‘œë¡œ, ë„ˆë¬´ ë§ì€ í´ëŸ¬ìŠ¤í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "- ë”°ë¼ì„œ, ì´ í•¨ìˆ˜ëŠ” ë³µì¡ì„±ì„ ìµœì†Œí™”í•˜ë©´ì„œë„ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ëŠ”ë° ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(Gaussian Mixture Model)ì„ ì‚¬ìš©í•˜ì—¬ ë² ì´ì§€ì•ˆ ì •ë³´ ê¸°ì¤€(BIC)ì„ í†µí•´ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - embeddings: numpy ë°°ì—´ë¡œì„œì˜ ì…ë ¥ ì„ë² ë”©.\n",
    "    - max_clusters: ê³ ë ¤í•  ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜.\n",
    "    - random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - ë°œê²¬ëœ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))  # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ì™€ ì„ë² ë”©ì˜ ê¸¸ì´ ì¤‘ ì‘ì€ ê°’ì„ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ì„¤ì •\n",
    "    n_clusters = np.arange(1, max_clusters)            # 1ë¶€í„° ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ê¹Œì§€ì˜ ë²”ìœ„ë¥¼ ìƒì„±\n",
    "    bics = []                                          # BIC ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    for n in n_clusters: # ê° í´ëŸ¬ìŠ¤í„° ìˆ˜ì— ëŒ€í•´ ë°˜ë³µ\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state) # ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        gm.fit(embeddings)              # ì„ë² ë”©ì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ\n",
    "        bics.append(gm.bic(embeddings)) # í•™ìŠµëœ ëª¨ë¸ì˜ BIC ì ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    return n_clusters[np.argmin(bics)]  # BIC ì ìˆ˜ê°€ ê°€ì¥ ë‚®ì€ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. ì£¼ì–´ì§„ ì„ë² ë”© ë°ì´í„°ì— ëŒ€í•´ GMMì„ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "- ê° ë°ì´í„° í¬ì¸íŠ¸(ì„ë² ë”©)ê°€ íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•˜ê³ , í™•ë¥  ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ì—¬ ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "- ì´ ê³¼ì •ì€ í™•ë¥  ì„ê³„ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n",
    "- ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì´ì „ì— êµ¬í˜„í•´ë‘” `get_optimal_clusters`í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "- ê²°ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³ , ì…ë ¥ëœ ì„ë² ë”©ì— ëŒ€í•´ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "- ê° ì„ë² ë”©ì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„° í• ë‹¹ í™•ë¥ ì„ ê³„ì‚°í•˜ê³ , ì´ í™•ë¥ ì´ ì£¼ì–´ì§„ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° í•´ë‹¹ ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "- í•¨ìˆ˜ëŠ” ìµœì¢…ì ìœ¼ë¡œ ì„ë² ë”©ì˜ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ê³¼ ê²°ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ íŠœí”Œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `GMM_cluster(embeddings, threshold, random_state) -> labels, n_clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    í™•ë¥  ì„ê³„ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸(GMM)ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ë§í•©ë‹ˆë‹¤.\n",
    "\n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - embeddings   : numpy ë°°ì—´ë¡œì„œì˜ ì…ë ¥ ì„ë² ë”©.\n",
    "    - threshold    : ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•˜ê¸° ìœ„í•œ í™•ë¥  ì„ê³„ê°’.\n",
    "    - random_state : ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ê³¼ ê²°ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” íŠœí”Œ.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)  # ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ êµ¬í•©ë‹ˆë‹¤.\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state) # ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "    gm.fit(embeddings)                                         # ì„ë² ë”©ì— ëŒ€í•´ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "    probs = gm.predict_proba(embeddings)                       # ì„ë² ë”©ì´ ê° í´ëŸ¬ìŠ¤í„°ì— ì†í•  í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs] # ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” í™•ë¥ ì„ ê°€ì§„ í´ëŸ¬ìŠ¤í„°ë¥¼ ë ˆì´ë¸”ë¡œ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    return labels, n_clusters                                  # ë ˆì´ë¸”ê³¼ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-5. ê³ ì°¨ì› ì„ë² ë”© ë°ì´í„°ë¥¼ ë‹¤ë‹¨ê³„ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "- ì°¨ì› ì¶•ì†Œ, ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§, ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ë³´ë‹¤ ì„¸ë¶„í™”ëœ ë°©ì‹ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ í•©ë‹ˆë‹¤.\n",
    "- ì´ ê³¼ì •ì€ ê³ ì°¨ì› ë°ì´í„°ì˜ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ë‹¤ì–‘í•œ ê·œëª¨ì—ì„œì˜ íŒ¨í„´ì„ í¬ì°©í•˜ëŠ”ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `perform_clustering(embeddings, dim, threshold) -> List[np.ndarray]`\n",
    "\n",
    "> ë™ì‘ ê³¼ì •\n",
    "1. ì„ë² ë”© í¬ê¸° í™•ì¸\n",
    "    - ì…ë ¥ëœ ì„ë² ë”©ì˜ ìˆ˜ê°€ `dim`+1 ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤ë©´, ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•˜ê³  ê¸°ë³¸ì ìœ¼ë¡œ ê° ì„ë² ë”©ì„ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "2. ê¸€ë¡œë²Œ ì°¨ì› ì¶•ì†Œ\n",
    "    - **UMAP** ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì„ë² ë”©ì˜ ì°¨ì›ì„ `dim`ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ë” ìš©ì´í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "    - `global_cluster_embeddings` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "3. ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§\n",
    "    - ì¶•ì†Œëœ ì„ë² ë”©ì— ëŒ€í•´ **GMM**ì„ ì‚¬ìš©í•˜ì—¬ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    - `GMM_cluster` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ê³ , ê° ì„ë² ë”©ì´ ì†í•œ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "4. ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§\n",
    "    - ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì„ë² ë”©ì— ëŒ€í•´ ì¶”ê°€ì ì¸ ë¡œì»¬ ì°¨ì› ì¶•ì†Œ ë° í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    - ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì„ë² ë”©ë“¤ì„ ì¶”ì¶œí•œ í›„, ë‹¤ì‹œ **UMAP**ì„ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œë¥¼ í•˜ê³ , **GMM**ì„ ì‚¬ìš©í•˜ì—¬ `local_cluster_embeddings` í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•´ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    - ê° ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì„ë² ë”©ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„° IDë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "5. í´ëŸ¬ìŠ¤í„° ID í• ë‹¹\n",
    "    - ê° ì„ë² ë”©ì— ëŒ€í•´ í• ë‹¹ëœ ê¸€ë¡œë²Œ ë° ë¡œì»¬ í´ëŸ¬ìŠ¤í„° IDë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ì„ë² ë”©ì´ ì†í•˜ëŠ” í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.\n",
    "    - ì´ ê³¼ì •ì—ì„œ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ìµœì¢… í´ëŸ¬ìŠ¤í„° IDë¡œ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "6. ê²°ê³¼ ë°˜í™˜\n",
    "    - ê° ì„ë² ë”©ì´ ì†í•˜ëŠ” ìµœì¢… í´ëŸ¬ìŠ¤í„° ID ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ë°°ì—´ì€ ëª¨ë“  ì„ë² ë”©ì— ëŒ€í•´ ê¸€ë¡œë²Œ ë° ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ê²°í•©í•œ IDë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    ì„ë² ë”©ì— ëŒ€í•´ ì°¨ì› ì¶•ì†Œ, ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§, ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œì˜ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - embeddings : numpy ë°°ì—´ë¡œ ëœ ì…ë ¥ ì„ë² ë”©.\n",
    "    - dim        : UMAP ì¶•ì†Œë¥¼ ìœ„í•œ ëª©í‘œ ì°¨ì›.\n",
    "    - threshold  : GMMì—ì„œ ì„ë² ë”©ì„ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•˜ê¸° ìœ„í•œ í™•ë¥  ì„ê³„ê°’.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - ê° ì„ë² ë”©ì˜ í´ëŸ¬ìŠ¤í„° IDë¥¼ í¬í•¨í•˜ëŠ” numpy ë°°ì—´ì˜ ë¦¬ìŠ¤íŠ¸.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ë•Œ í´ëŸ¬ìŠ¤í„°ë§ì„ í”¼í•©ë‹ˆë‹¤.\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # ê¸€ë¡œë²Œ ì°¨ì› ì¶•ì†Œ\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë§\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # ê° ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ë¥¼ ìˆœíšŒí•˜ë©° ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰\n",
    "    for i in range(n_global_clusters):\n",
    "        # í˜„ì¬ ê¸€ë¡œë²Œ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ì„ë² ë”© ì¶”ì¶œ\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # ì‘ì€ í´ëŸ¬ìŠ¤í„°ëŠ” ì§ì ‘ í• ë‹¹ìœ¼ë¡œ ì²˜ë¦¬\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # ë¡œì»¬ ì°¨ì› ì¶•ì†Œ ë° í´ëŸ¬ìŠ¤í„°ë§\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # ë¡œì»¬ í´ëŸ¬ìŠ¤í„° ID í• ë‹¹, ì´ë¯¸ ì²˜ë¦¬ëœ ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì¡°ì •\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-6. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¬¸ì„œì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "- embd ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•œ í›„, í´ëŸ¬ìŠ¤í„°ë§í•˜ê³  ì›ë³¸ í…ìŠ¤íŠ¸, í•´ë‹¹ ì„ë² ë”©, ê·¸ë¦¬ê³  í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ í¬í•¨í•˜ëŠ” `pandas.DataFrame`ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `embed(texts) -> numpy.ndarray`\n",
    "\n",
    "> ë™ì‘ ê³¼ì •\n",
    "- ì…ë ¥ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ëª©ë¡(`texts`)ì„ ë°›ìŠµë‹ˆë‹¤.\n",
    "- `embd` ê°ì²´ì˜ `embed_documents` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- ìƒì„±ëœ ì„ë² ë”©ì„ `numpy.ndarray` í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    ì´ í•¨ìˆ˜ëŠ” `embd` ê°ì²´ê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•˜ë©°, ì´ ê°ì²´ëŠ” í…ìŠ¤íŠ¸ ëª©ë¡ì„ ë°›ì•„ ê·¸ ì„ë² ë”©ì„ ë°˜í™˜í•˜ëŠ” `embed_documents` ë©”ì†Œë“œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - texts: List[str], ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ëª©ë¡ì…ë‹ˆë‹¤.\n",
    "    \n",
    "    ë°˜í™˜ê°’:\n",
    "    - numpy.ndarray: ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì— ëŒ€í•œ ì„ë² ë”© ë°°ì—´ì…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(\n",
    "        texts\n",
    "    )  # í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    text_embeddings_np = np.array(text_embeddings) # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    return text_embeddings_np                      # ì„ë² ë”©ëœ numpy ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-7. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ êµ¬ì¡°ì  ë¶„ì„, ê·¸ë£¹í™”ë¥¼ ìœ„í•œ í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬í˜„\n",
    "\n",
    "- ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ , ìƒì„±ëœ ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ì›ë³¸ í…ìŠ¤íŠ¸, í•´ë‹¹ ì„ë² ë”©, ê·¸ë¦¬ê³  í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ í¬í•¨í•˜ëŠ” **pandas.DataFrame**ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `embed_cluster_texts(texts) -> df`\n",
    "\n",
    "> ë™ì‘ ê³¼ì •\n",
    "1. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± (ì„ë² ë”© ìƒì„±ì„ ìœ„í•´ `embed()`ì‚¬ìš©)\n",
    "2. ìƒì„±ëœ ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰, ì´ ê³¼ì •ì€ ì‚¬ì „ì— ì •ì˜ëœ `perform_clustering()`í•¨ìˆ˜ë¥¼ ì‚¬ìš©\n",
    "3. ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ **pandas.DataFrame**ì„ ì´ˆê¸°í™”\n",
    "4. DataFrameì— ì›ë³¸ í…ìŠ¤íŠ¸, ì„ë² ë”© ë¦¬ìŠ¤íŠ¸, í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ ê°ê° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ëª©ë¡ì„ ì„ë² ë”©í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬, í…ìŠ¤íŠ¸, ê·¸ë“¤ì˜ ì„ë² ë”©, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì´ í¬í•¨ëœ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    ì´ í•¨ìˆ˜ëŠ” ì„ë² ë”© ìƒì„±ê³¼ í´ëŸ¬ìŠ¤í„°ë§ì„ ë‹¨ì¼ ë‹¨ê³„ë¡œ ê²°í•©í•©ë‹ˆë‹¤. ì„ë² ë”©ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” `perform_clustering` í•¨ìˆ˜ì˜ ì‚¬ì „ ì •ì˜ëœ ì¡´ì¬ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - texts: List[str], ì²˜ë¦¬ë  í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ëª©ë¡ì…ë‹ˆë‹¤.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - pandas.DataFrame: ì›ë³¸ í…ìŠ¤íŠ¸, ê·¸ë“¤ì˜ ì„ë² ë”©, ê·¸ë¦¬ê³  í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì´ í¬í•¨ëœ DataFrameì…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts) # ì„ë² ë”© ìƒì„±\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # ì„ë² ë”©ì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰\n",
    "    df = pd.DataFrame()                   # ê²°ê³¼ë¥¼ ì €ì¥í•  DataFrame ì´ˆê¸°í™”\n",
    "    df[\"text\"] = texts                    # ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "    df[\"embd\"] = list(text_embeddings_np) # DataFrameì— ë¦¬ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ì €ì¥\n",
    "    df[\"cluster\"] = cluster_labels        # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì €ì¥\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-8. í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë‹¨ì¼ ë¬¸ìì—´ì„ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜ êµ¬í˜„\n",
    "\n",
    "- **pandas**ì˜ **DataFrame**ì—ì„œ `text` ì»¬ëŸ¼ì— ìˆëŠ” ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë‹¨ì¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.\n",
    "- ê° í…ìŠ¤íŠ¸ ë¬¸ì„œëŠ” íŠ¹ì • êµ¬ë¶„ì(\"--- --- \\n --- --- \")ë¡œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ìµœì¢…ì ìœ¼ë¡œ ì—°ê²°ëœ í•˜ë‚˜ì˜ í° ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "- ì´ í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `fmt_txt(df) -> str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    DataFrameì— ìˆëŠ” í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ í¬ë§·í•©ë‹ˆë‹¤.\n",
    "\n",
    "    íŒŒë¼ë¯¸í„°:\n",
    "    - df: 'text' ì—´ì— í¬ë§·í•  í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ í¬í•¨ëœ DataFrame.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - ëª¨ë“  í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ íŠ¹ì • êµ¬ë¶„ìë¡œ ê²°í•©ëœ ë‹¨ì¼ ë¬¸ìì—´.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()  # 'text' ì—´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    return \"--- --- \\n --- --- \".join(\n",
    "        unique_txt\n",
    "    )  # í…ìŠ¤íŠ¸ ë¬¸ì„œë“¤ì„ íŠ¹ì • êµ¬ë¶„ìë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-9. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±, í´ëŸ¬ìŠ¤í„°ë§, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° ë‚´ í…ìŠ¤íŠ¸ ìš”ì•½ì„ ë‹¨ê³„ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
    "\n",
    "- í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ê³ , í´ëŸ¬ìŠ¤í„°ë§í•˜ë©°, ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "- ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìœ ì‚¬ì„±ì— ê¸°ë°˜í•œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ `df_clusters` ë°ì´í„°í”„ë ˆì„ì„ ê²°ê³¼ë¡œ í•©ë‹ˆë‹¤. ì´ ë°ì´í„°í”„ë ˆì„ì—ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸, ì„ë² ë”©, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° í• ë‹¹ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "- í´ëŸ¬ìŠ¤í„° í• ë‹¹ì„ ì‰½ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ í•­ëª©ì„ í™•ì¥í•©ë‹ˆë‹¤. ê° í–‰ì€ í…ìŠ¤íŠ¸, ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë¥¼ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n",
    "- í™•ì¥ëœ ë°ì´í„°í”„ë ˆì„ì—ì„œ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ ì¶”ì¶œí•˜ê³ , ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ìš”ì•½ì€ `df_summary` ë°ì´í„°í”„ë ˆì„ì— ì €ì¥ë©ë‹ˆë‹¤. ì´ ë°ì´í„°í”„ë ˆì„ì€ ê° í´ëŸ¬ìŠ¤í„°ì˜ ìš”ì•½, ì§€ì •ëœ ì„¸ë¶€ ìˆ˜ì¤€, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "- ìµœì¢…ì ìœ¼ë¡œ, í•¨ìˆ˜ëŠ” ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ì„ í¬í•¨í•˜ëŠ” íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì€ ì›ë³¸ í…ìŠ¤íŠ¸, ì„ë² ë”©, í´ëŸ¬ìŠ¤í„° í• ë‹¹ ì •ë³´ë¥¼ í¬í•¨í•˜ë©°, ë‘ ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì€ ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ìš”ì•½ê³¼ í•´ë‹¹ ì„¸ë¶€ ìˆ˜ì¤€, í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "- êµ¬í˜„ í•¨ìˆ˜ : `embd_cluster_summarize_texts(texts: List[str], level: int) -> Tuple[pd.DataFrame, pd.DataFrame]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë¨¼ì € í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ ,\n",
    "    ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•œ ë‹¤ìŒ, í´ëŸ¬ìŠ¤í„° í• ë‹¹ì„ í™•ì¥í•˜ì—¬ ì²˜ë¦¬ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê³  ê° í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "\n",
    "    ë§¤ê°œë³€ìˆ˜:\n",
    "    - texts: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡ì…ë‹ˆë‹¤.\n",
    "    - level: ì²˜ë¦¬ì˜ ê¹Šì´ë‚˜ ì„¸ë¶€ ì‚¬í•­ì„ ì •ì˜í•  ìˆ˜ ìˆëŠ” ì •ìˆ˜ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ì„ í¬í•¨í•˜ëŠ” íŠœí”Œ:\n",
    "      1. ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„(`df_clusters`)ì€ ì›ë³¸ í…ìŠ¤íŠ¸, ê·¸ë“¤ì˜ ì„ë² ë”©, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° í• ë‹¹ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "      2. ë‘ ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„(`df_summary`)ì€ ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ìš”ì•½, ì§€ì •ëœ ì„¸ë¶€ ìˆ˜ì¤€, ê·¸ë¦¬ê³  í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ 'text', 'embd', 'cluster' ì—´ì´ ìˆëŠ” ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # í´ëŸ¬ìŠ¤í„°ë¥¼ ì‰½ê²Œ ì¡°ì‘í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ì„ í™•ì¥í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
    "    expanded_list = []\n",
    "\n",
    "    # ë°ì´í„°í”„ë ˆì„ í•­ëª©ì„ ë¬¸ì„œ-í´ëŸ¬ìŠ¤í„° ìŒìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì²˜ë¦¬ë¥¼ ê°„ë‹¨í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # í™•ì¥ëœ ëª©ë¡ì—ì„œ ìƒˆ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # ì²˜ë¦¬ë¥¼ ìœ„í•´ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„° ì‹ë³„ìë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # ìš”ì•½\n",
    "    template = \"\"\"ì—¬ê¸° LangChain í‘œí˜„ ì–¸ì–´ ë¬¸ì„œì˜ í•˜ìœ„ ì§‘í•©ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    LangChain í‘œí˜„ ì–¸ì–´ëŠ” LangChainì—ì„œ ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì œê³µëœ ë¬¸ì„œì˜ ìì„¸í•œ ìš”ì•½ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.\n",
    "    \n",
    "    ë¬¸ì„œ:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # ê° í´ëŸ¬ìŠ¤í„° ë‚´ì˜ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½ì„ ìœ„í•´ í¬ë§·íŒ…í•©ë‹ˆë‹¤.\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # ìš”ì•½, í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ë° ë ˆë²¨ì„ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-10 í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½í•˜ëŠ” ê³¼ì •ì„ êµ¬í˜„í•œ í•¨ìˆ˜\n",
    "\n",
    "- ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½í•˜ì—¬ ê° ë‹¨ê³„ë³„ë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- í•¨ìˆ˜ëŠ” ìµœëŒ€ ì§€ì •ëœ ì¬ê·€ ë ˆë²¨ê¹Œì§€ ì‹¤í–‰ë˜ê±°ë‚˜, ìœ ì¼í•œ í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ê°€ 1ì´ ë  ë•Œê¹Œì§€ ë°˜ë³µë©ë‹ˆë‹¤.\n",
    "- ê° ì¬ê·€ ë‹¨ê³„ì—ì„œëŠ” í˜„ì¬ ë ˆë²¨ì˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ ìš”ì•½ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ í˜•íƒœë¡œ ë°˜í™˜í•˜ê³ , ì´ë¥¼ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- ë§Œì•½ í˜„ì¬ ë ˆë²¨ì´ ìµœëŒ€ ì¬ê·€ ë ˆë²¨ë³´ë‹¤ ì‘ê³ , ìœ ì¼í•œ í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ê°€ 1ë³´ë‹¤ í¬ë‹¤ë©´, í˜„ì¬ ë ˆë²¨ì˜ ìš”ì•½ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë ˆë²¨ì˜ ì…ë ¥ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ì¬ê·€ì ìœ¼ë¡œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "- ìµœì¢…ì ìœ¼ë¡œ ê° ë ˆë²¨ë³„ í´ëŸ¬ìŠ¤í„° ë°ì´í„°í”„ë ˆì„ê³¼ ìš”ì•½ ë°ì´í„°í”„ë ˆì„ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    ì§€ì •ëœ ë ˆë²¨ê¹Œì§€ ë˜ëŠ” ê³ ìœ  í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ê°€ 1ì´ ë  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§, ìš”ì•½í•˜ì—¬\n",
    "    ê° ë ˆë²¨ì—ì„œì˜ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "    ë§¤ê°œë³€ìˆ˜:\n",
    "    - texts: List[str], ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ë“¤.\n",
    "    - level: int, í˜„ì¬ ì¬ê·€ ë ˆë²¨ (1ì—ì„œ ì‹œì‘).\n",
    "    - n_levels: int, ì¬ê·€ì˜ ìµœëŒ€ ê¹Šì´.\n",
    "\n",
    "    ë°˜í™˜ê°’:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], ì¬ê·€ ë ˆë²¨ì„ í‚¤ë¡œ í•˜ê³  í•´ë‹¹ ë ˆë²¨ì—ì„œì˜ í´ëŸ¬ìŠ¤í„° DataFrameê³¼ ìš”ì•½ DataFrameì„ í¬í•¨í•˜ëŠ” íŠœí”Œì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ì‚¬ì „.\n",
    "    \"\"\"\n",
    "    results = {}  # ê° ë ˆë²¨ì—ì„œì˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ì‚¬ì „\n",
    "\n",
    "    # í˜„ì¬ ë ˆë²¨ì— ëŒ€í•´ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§, ìš”ì•½ ìˆ˜í–‰\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # í˜„ì¬ ë ˆë²¨ì˜ ê²°ê³¼ ì €ì¥\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # ì¶”ê°€ ì¬ê·€ê°€ ê°€ëŠ¥í•˜ê³  ì˜ë¯¸ê°€ ìˆëŠ”ì§€ ê²°ì •\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # ë‹¤ìŒ ë ˆë²¨ì˜ ì¬ê·€ ì…ë ¥ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½ ì‚¬ìš©\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # ë‹¤ìŒ ë ˆë²¨ì˜ ê²°ê³¼ë¥¼ í˜„ì¬ ê²°ê³¼ ì‚¬ì „ì— ë³‘í•©\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ëª¨ë¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„ë² ë”© ëª¨ë¸ì€ `intfloat/multilingual-e5-base`ì„ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤. <br>\n",
    "**Cached Embedding** ì„ ì‚¬ìš©í•˜ì—¬ í•œë²ˆ ê³„ì‚°ëœ ì„ë² ë”©ì„ ì €ì¥í•´ ë‘ì—ˆë‹¤ê°€, ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì´ì „ì— ê³„ì‚°ëœ ì„ë² ë”© ê°’ì„ ê³„ì† í™œìš©í•˜ì—¬ ì¤‘ë³µ ê³„ì‚°ì„ í”¼í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë¦¬ì†ŒìŠ¤ì„ ì¢€ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ì—°ì‚° ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_embeddings(model_path=\"intfloat/multilingual-e5-base\"):\n",
    "    \"\"\"ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë° ìºì‹œ ì‚¬ìš©\"\"\"\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embd = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    \n",
    "    store = LocalFileStore(\"./cache/\")\n",
    "    \n",
    "    # Cache Embedding ì‚¬ìš©\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings=embd, \n",
    "        document_embedding_cache=store, \n",
    "        namespace=model_path\n",
    "    )\n",
    "    \n",
    "    return cached_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì „ì— `sh2orc/Llama-3.1-Korean-8B-Instruct`(Llama3 ëª¨ë¸ í•œêµ­ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸)ì„ ëŒ€ìƒìœ¼ë¡œ GPTQ ì–‘ìí™” ì‹œì¼œì¤€ ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline():\n",
    "    # ëª¨ë¸ ID (Hugging Face Hubì—ì„œ ê°€ì ¸ì˜¨ ì–‘ìí™”ëœ ëª¨ë¸ ID)\n",
    "    model_id = \"ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit\"\n",
    "\n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì„¤ì •\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ ë° ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"#### [ model ] ####\\n{model}\\n###################\")\n",
    "\n",
    "    # HuggingFacePipeline ê°ì²´ ìƒì„±\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=450,\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ë°ì´í„°ì…‹ ë¡œë“œ ë° ë²¡í„° DB ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n",
    "    \"\"\"PDF íŒŒì¼ë¡œë“œ, í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    # PDF íŒŒì¼ ì—´ê¸°\n",
    "    doc = fitz.open(file_path)\n",
    "    doc_texts = []\n",
    "    \n",
    "    # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        doc_texts.append(Document(page_content=text, metadata={\"page\": page_num}))\n",
    "\n",
    "    return doc_texts    \n",
    "\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"ê²½ë¡œ ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\"\"\"\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "\n",
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°í”„ë ˆì„ì—ì„œ PDF ì²˜ë¦¬, \n",
    "    ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³ , ê° íŒŒì¼ì— ëŒ€í•´ ë²¡í„° DBì™€ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    ë”•ì…”ë„ˆë¦¬ì— pdfëª…ì„ í‚¤ë¡œí•´ì„œ DB, retriever ì €ì¥\n",
    "    \"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # ê²½ë¡œ ì •ê·œí™” ë° ì ˆëŒ€ ê²½ë¡œ ìƒì„±\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF ì²˜ë¦¬ \n",
    "        docs_texts = process_pdf(full_path)\n",
    "        \n",
    "        # íŠ¸ë¦¬ êµ¬ì¶•\n",
    "        leaf_texts = docs_texts  # ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¦¬í”„ í…ìŠ¤íŠ¸ë¡œ ì„¤ì •\n",
    "        results = recursive_embed_cluster_summarize(\n",
    "            leaf_texts, level=1, n_levels=3\n",
    "        )  # ì¬ê·€ì ìœ¼ë¡œ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ìŒ\n",
    "        \n",
    "        # leaf_textsë¥¼ ë³µì‚¬í•˜ì—¬ all_textsë¥¼ ì´ˆê¸°í™”\n",
    "        all_texts = leaf_texts.copy()\n",
    "        \n",
    "        # ê° ë ˆë²¨ì˜ ìš”ì•½ì„ ì¶”ì¶œí•˜ì—¬ all_textsì— ì¶”ê°€\n",
    "        for level in sorted(results.keys()):\n",
    "            summaries = results[level][1][\"summaries\"].tolist()  # í˜„ì¬ ë ˆë²¨ì˜ DataFrameì—ì„œ ìš”ì•½ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "            all_texts.extend(summaries) # í˜„ì¬ ë ˆë²¨ì˜ ìš”ì•½ì„ all_textsì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        # FAISS ë²¡í„° DB ìƒì„±\n",
    "        embeddings = setup_embeddings()\n",
    "        vectorstore = FAISS.from_texts(texts=all_texts, embedding=embeddings)\n",
    "        \n",
    "        # ë¡œì»¬ì— FAISS DB ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ìˆë‹¤ë©´ ë³‘í•©\n",
    "        DB_INDEX = f\"RAPTOR_{pdf_title}\"\n",
    "        if os.path.exists(DB_INDEX):\n",
    "            local_index = FAISS.load_local(DB_INDEX, embeddings)\n",
    "            local_index.merge_from(vectorstore)\n",
    "            local_index.save_local(DB_INDEX)\n",
    "        else:\n",
    "            vectorstore.save_local(folder_path=DB_INDEX)\n",
    "        \n",
    "        # Retriever ìƒì„±\n",
    "        retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'fetch_k': 8})\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        pdf_databases[pdf_title] = {\n",
    "            'db': vectorstore,\n",
    "            'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LLM ì¶”ë¡ , ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    # ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì„ ì´ì–´ë¶™ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def generate_answers(df, pdf_databases, llm):\n",
    "    \"\"\"DataFrameì˜ ê° ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "        source = normalize_string(row['Source'])\n",
    "        question = row['Question']\n",
    "\n",
    "        # ì •ê·œí™”ëœ í‚¤ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰\n",
    "        normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "        retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "        # # RAG ì²´ì¸ êµ¬ì„±\n",
    "        # template = \"\"\"\n",
    "        # ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n",
    "        # {context}\n",
    "\n",
    "        # ì§ˆë¬¸: {question}\n",
    "        \n",
    "        # ì£¼ì–´ì§„ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€í•  ë•Œ ì§ˆë¬¸ì˜ ì£¼ì–´ë¥¼ ì¨ì£¼ì„¸ìš”.\n",
    "        # ë‹µë³€:\n",
    "        # \"\"\"\n",
    "        # prompt = PromptTemplate.from_template(template)\n",
    "        # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        \n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # ë‹µë³€ ì¶”ë¡ \n",
    "        full_response = rag_chain.invoke(question)\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        results.append({\n",
    "            \"Source\": row['Source'],\n",
    "            \"Source_path\": row['Source_path'],\n",
    "            \"Question\": question,\n",
    "            \"Answer\": full_response\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. ìµœì¢… í†µí•© ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-1. Vector DB ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv \u001b[34mtest_source\u001b[m\u001b[m           \u001b[34mtrain_source\u001b[m\u001b[m\n",
      "test.csv              train.csv\n"
     ]
    }
   ],
   "source": [
    "# google drive\n",
    "# !ls /content/drive/MyDrive/Contest/Dacon_Financial_Search/open/\n",
    "\n",
    "# local\n",
    "!ls /Users/ormor/root/contest/dacon_financial_search/open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "embeddings = setup_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF íŒŒì¼ë“¤ ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„±\n",
    "\n",
    "# google drive\n",
    "# base_directory = r'/content/drive/MyDrive/Contest/Dacon_Financial_Search/open' # pdf source ê²½ë¡œ\n",
    "# df = pd.read_csv(r'/content/drive/MyDrive/Contest/Dacon_Financial_Search/open/test.csv') # csv íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "# local\n",
    "base_directory = f'/Users/ormor/root/contest/dacon_financial_search/open' # pdf source ê²½ë¡œ\n",
    "df = pd.read_csv(f'{base_directory}/test.csv') # csv íŒŒì¼ ê²½ë¡œ\n",
    "pdf_databases = process_pdfs_from_dataframe(df, base_directory, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2. ì¶”ë¡  ì‹¤í–‰, ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith ì„¤ì •\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •, ì½œì†”ì— Langchain Api Key ì…ë ¥\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(prompt=\"Enter your LangChain API key: \")\n",
    "\n",
    "# ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = setup_llm_pipeline()\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "results = generate_answers(df, pdf_databases, llm) # list ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ì œì¶œìš© íŒŒì¼ ìƒì„± ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ\n",
    "submit_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# ìƒì„±ëœ ë‹µë³€ì„ ì œì¶œ DataFrameì— ì¶”ê°€\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"ë°ì´ì½˜\") # ëª¨ë¸ì—ì„œ ë¹ˆ ê°’ (NaN) ìƒì„± ì‹œ ì±„ì ì— ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìŒ [ ì£¼ì˜ ]\n",
    "\n",
    "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "submit_df.to_csv(\"./baseline_submission.csv\", encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
