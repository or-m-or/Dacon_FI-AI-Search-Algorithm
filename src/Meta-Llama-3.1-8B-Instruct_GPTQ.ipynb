{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Meta-Llama-3.1-8B-Instruct 모델 GPTQ 양자화하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers peft accelerate optimum\n",
    "!pip install datasets==2.15.0\n",
    "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n",
    "!pip install huggingface_hub\n",
    "!pip install langchain\n",
    "!pip install transformers[torch] -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 지원되는 데이터셋을 전달하여 모델 양자화하기\n",
    "- 'wikitest2' 데이터셋 사용\n",
    "- 추후 재정정보 데이터셋 전처리 후 사용고려\n",
    "- 4비트 정밀도로 양자화 (지원 : 2,4,6,8)\n",
    "\n",
    "> **전처리 시 고려사항**\n",
    "1. PyPDF2, pdfplumbler, pdfminer 같은 라이브러리로 pdf에서 텍스트 추출\n",
    "2. 불필요한 공백, 페이지 번호, 제목 등 필요하지 않은 정보 제거\n",
    "3. 순서가 맞지 않는 문장 재구성, 정제\n",
    "4. 데이터셋 형태로 저장(JSON, CSV, TXT 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "# 모델 ID\n",
    "model_id = \"sh2orc/Llama-3.1-Korean-8B-Instruct\" # 베이스 Llama 3 + 한국어 파인튜닝 모델\n",
    "\n",
    "# GPTQ 양자화 설정\n",
    "quantization_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,       # 일반적으로 사용되는 그룹 크기\n",
    "    dataset=\"wikitext2\",  # 양자화에 사용할 기본 데이터셋, 추후 제공받은 데이터셋을 전처리하여 넣어주는 것도 고려하기\n",
    "    desc_act=False\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 모델 로드 및 양자화\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",         # 자동으로 가능한 장치에 할당\n",
    "    torch_dtype=torch.float16  # 메모리 절약을 위해 float16 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 레이어의 속성을 확인하여 모델이 올바르게 정량화되었는지 확인할 수 있으며, 여기에는 torch.int32 dtype에 있어야 하는 qweight 및 qzeros 속성이 포함되어 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 잘 동작하는지 확인하기 위해 양자화된 모델에 대해 추론을 수행해 보겠습니다. (트랜스포머와 동일한 API를 사용할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "text = \"안녕\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = quant_model.generate(**inputs)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 양자화된 모델 허깅페이스에 업로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Hugging Face Hub 로그인 (최초 1회 실행 필요)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화된 모델을 Hugging Face Hub에 업로드\n",
    "quant_model.push_to_hub(\"ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit\")\n",
    "tokenizer.push_to_hub(\"ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 허깅페이스에 업로드한 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "def setup_llm_pipeline():\n",
    "    # 모델 ID (Hugging Face Hub에서 가져온 양자화된 모델 ID)\n",
    "    model_id = \"ormor/Llama-3.1-Korean-8B-Instruct-GPTQ-4bit\"\n",
    "\n",
    "    # 토크나이저 로드 및 설정\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    # 모델 로드 및 양자화 설정 적용\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"#### [ model ] ####\\n{model}\\n###################\")\n",
    "\n",
    "    # HuggingFacePipeline 객체 생성\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=450,\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이프라인 설정 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 설정\n",
    "hf_pipeline = setup_llm_pipeline()\n",
    "\n",
    "# 텍스트 생성 예시\n",
    "text = \"안녕\"\n",
    "result = hf_pipeline(text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
