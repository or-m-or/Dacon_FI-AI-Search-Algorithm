{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd26cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 가속화 및 메모리 관리\n",
    "%pip install accelerate\n",
    "%pip install bitsandbytes\n",
    "\n",
    "%pip install 'autoawq>=0.1.7'  \n",
    "%pip install datasets\n",
    "%pip install torch==2.2.0\n",
    "%pip install torchvision==0.17.0\n",
    "%pip install transformers -U\n",
    "%pip install sentence-transformers \n",
    "\n",
    "# 벡터 데이터베이스 관련 라이브러리\n",
    "# %pip install faiss-cpu\n",
    "%pip install faiss-gpu\n",
    " \n",
    "# 데이터\n",
    "%pip install tiktoken\n",
    "%pip install pymupdf4llm \n",
    "%pip install pandas \n",
    "\n",
    "# langchain\n",
    "%pip install langchain \n",
    "%pip install langchain-community \n",
    "%pip install langchain-huggingface\n",
    "%pip install langchain-anthropic\n",
    "\n",
    "%pip install python-dotenv \n",
    "%pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6333b-8ec4-4879-adc1-c8ecd630e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import pymupdf4llm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from accelerate import Accelerator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chains import create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.utils import CacheBackedEmbeddings\n",
    "from langchain.store import LocalFileStore\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "# 비동기 이벤트 루프를 재설정하거나 중첩할 수 있도록 허용\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# CUDA 메모리 할당이 필요할 때마다 메모리 블록을 확장\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 시드 설정\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9a4ae-ae46-4ee1-b7b1-f36b50614198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 클래스\n",
    "class Config:\n",
    "    def __init__(self, llm_name: str, embedding_name: str, concept: str) -> None:        \n",
    "        # 모델별 설정 딕셔너리\n",
    "        self.model_config = {\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\": {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"rtzr/ko-gemma-2-9b-it\": {\n",
    "                \"quantization_config\": self.get_quantization_config(),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 450,\n",
    "            },\n",
    "            \"jjjguz/Llama-3.1-Korean-8B-Instruct-v1\": {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"mindsignal/rtzr-ko-gemma-2-9b-it-4bit-financesinfo-ver1\": {\n",
    "                \"quantization_config\": self.get_quantization_config(),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 450,\n",
    "            },\n",
    "            \"mindsignal/upstage-SOLAR-10.7B-Instruct-v1.0-4bit-financesinfo-ver1\": {\n",
    "                \"quantization_config\": self.get_quantization_config(),\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"max_token\": 512\n",
    "            },\n",
    "            \"upstage/SOLAR-10.7B-Instruct-v1.0\": {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Large Language Model\n",
    "        self.llm_name = llm_name\n",
    "        self.llm_config = self.model_config[self.llm_name]\n",
    "        self.llm_obj = self.setup_llm()\n",
    "        \n",
    "        # 임베딩 모델 설정\n",
    "        self.embedding_name = embedding_name\n",
    "        self.embedding_obj = self.setup_embedding()\n",
    "        \n",
    "        # document\n",
    "        self.chunk_size = 512\n",
    "        self.chunk_overlap = 32\n",
    "        self.bm25_w = 0.5\n",
    "        self.faiss_w = 0.5\n",
    "        \n",
    "        # Data Path\n",
    "        self.base_directory = \"open/\"\n",
    "        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "    \n",
    "    def get_quantization_config(self):\n",
    "        \"\"\"4-bit 양자화 설정을 반환하는 함수\"\"\"\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,  # 4-bit 양자화\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16  # 연산에 사용할 데이터 타입\n",
    "        )\n",
    "        \n",
    "    def setup_embedding(self):\n",
    "        \"\"\" 임베딩 모델 설정 \"\"\"\n",
    "        embed_id = self.embedding_name\n",
    "        \n",
    "        model_kwargs = {'device': 'cuda'}\n",
    "        encode_kwargs = {'normalize_embeddings': True}\n",
    "        embd = HuggingFaceEmbeddings(\n",
    "            model_name=embed_id,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        \n",
    "        store = LocalFileStore(\"./cache/\")\n",
    "        \n",
    "        # Cache Embedding 사용\n",
    "        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "            underlying_embeddings=embd, \n",
    "            document_embedding_cache=store, \n",
    "            namespace=embed_id\n",
    "        )\n",
    "        \n",
    "        return cached_embeddings\n",
    "    \n",
    "    def setup_llm(self):\n",
    "        \"\"\"LLM 설정 및 파이프라인 구성\"\"\"\n",
    "        model_id = self.llm_name\n",
    "        \n",
    "        # 토크나이저 로드 및 설정\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.use_default_system_prompt = False\n",
    "        \n",
    "        # 모델 로드 및 양자화 설정 적용\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=self.llm_config[\"quantization_config\"],\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 모델을 여러 GPU에 할당\n",
    "        accelerator = Accelerator()\n",
    "        model = accelerator.prepare(model)\n",
    "        \n",
    "        print(f\"#### [ model ] ####\\n{model}\\n###################\")\n",
    "        \n",
    "        # 스트리머를 설정하여 토큰이 생성될 때마다 출력\n",
    "        streamer = TextStreamer(tokenizer)\n",
    "\n",
    "        # HuggingFacePipeline 객체 생성\n",
    "        text_generation_pipeline = pipeline(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            task=\"text-generation\",\n",
    "            return_full_text=False,\n",
    "            max_new_tokens=450,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "\n",
    "        hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "        return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067231f-dff2-4628-b74a-e8735547ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "    def normalize_path(self, path):\n",
    "        \"\"\" Path 유니코드 정규화 \"\"\"\n",
    "        normalized_path = unicodedata.normalize('NFD', path)\n",
    "        logger.debug(f\"정규화된 경로: {normalized_path}\")\n",
    "        return normalized_path\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\" 텍스트 전처리 함수 \"\"\"\n",
    "        # 불필요한 공백 제거\n",
    "        text = text.strip()\n",
    "        # 마침표 뒤 및 \"----\" 전후의 줄바꿈을 제외한 모든 줄바꿈을 제거\n",
    "        text = re.sub(r'(?<!\\.)(?<!-----)(\\n|\\r\\n)(?!-----)', ' ', text)\n",
    "        # 공백 처리 및 불필요한 줄바꿈 제거\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def process_pdf(self, file_path: str) -> list:\n",
    "        \"\"\" PDF 파일 로드, 텍스트 추출 및 전처리 후 노드 생성 \"\"\"\n",
    "        logger.info(f\"PDF 처리 중: {file_path}\")\n",
    "        md_content = pymupdf4llm.to_markdown(file_path)\n",
    "        \n",
    "        md_content = self.preprocess_text(md_content)\n",
    "                \n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "        \n",
    "        md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "        md_chunks = md_header_splitter.split_text(md_content)\n",
    "        \n",
    "        # 텍스트 데이터를 문서 청크로 분할\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=self.config.chunk_size, chunk_overlap=self.config.chunk_overlap\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(md_chunks)\n",
    "        logger.info(f\"텍스트를 {len(chunks)}개의 청크로 분할 완료.\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def init_vector_db(self, pdf_files):\n",
    "        \"\"\" PDF 파일 리스트를 처리하고 벡터 데이터베이스 및 리트리버를 초기화 \"\"\"\n",
    "        pdf_databases = {}\n",
    "        unique_paths = df['Source_path'].unique()\n",
    "        \n",
    "        for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "            normalized_path = self.normalize_path(path)\n",
    "            full_path = os.path.normpath(os.path.join(self.config.base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "            \n",
    "            pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "            print(f\"Processing {pdf_title}...\")\n",
    "            \n",
    "            # PDF 처리 및 벡터 DB 생성\n",
    "            chunks = self.process_pdf(full_path)\n",
    "\n",
    "            vector_store = FAISS.from_documents(chunks, embedding=self.config.embedding_obj)\n",
    "            \n",
    "            bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "            faiss_retriever = vector_store.as_retriever()\n",
    "    \n",
    "            retriever = EnsembleRetriever(\n",
    "                retrievers=[bm25_retriever, faiss_retriever],\n",
    "                weights=[self.config.bm25_w, self.config.faiss_w],\n",
    "                search_type=\"mmr\",\n",
    "            )\n",
    "\n",
    "            # 결과 저장\n",
    "            pdf_databases[pdf_title] = {\n",
    "                'vector_store': vector_store,\n",
    "                'retriever': retriever\n",
    "            }\n",
    "            \n",
    "        return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1dbad-6ca1-4b68-ab8e-6555de9f5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 생성 및 답변 생성 프롬프트 템플릿\n",
    "question_prompt_template = \"\"\" \n",
    "다음 문서에서 이해할 수 있는 주요 질문을 생성하세요:\n",
    "\n",
    "문서 내용: \n",
    "{context}\n",
    "\"\"\"\n",
    "answer_prompt_template = \"\"\" \n",
    "당신은 재정 정책 전문가입니다. 주어진 정보를 바탕으로, 질문에 대해 정확히 답변하세요.\n",
    "\n",
    "답변은 간결하고 명확하게 작성하며, 주어진 질문에 대해 핵심만 답변하세요.\n",
    "단, 주어와 서술어를 사용하여 온전한 문장을 완성시켜 최대한 자연스럽게 답변해야 합니다.\n",
    "\n",
    "자연스럽게 답변하기 위해 아래 조건을 지켜주세요. :\n",
    "질문을 답변으로 생성하지 마세요.\n",
    "아래 정보의 내용을 그대로 출력하지 마세요.\n",
    "아래 정보의 내용을 참고해서 질문에 올바른 답변을 생성하세요.\n",
    "답변하기 전에 한번 더 생각해보고 답변하세요.\n",
    "기호나 공백을 제거하고 깔끔하게 답변하세요.\n",
    "\n",
    "다음 정보를 바탕으로 질문에 답하세요 : \n",
    "{context}\n",
    "\"\"\"\n",
    "question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", question_prompt_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", answer_prompt_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000f5ae-0182-4414-9364-6c184f416053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FISearch:\n",
    "    def __init__(self, config: Config, question_prompt, answer_prompt) -> None:\n",
    "        self.config = config\n",
    "        self.question_prompt = question_prompt\n",
    "        self.answer_prompt = answer_prompt\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        \"\"\" 유니코드 정규화 \"\"\"\n",
    "        return unicodedata.normalize('NFC', s)\n",
    "    \n",
    "    def format_docs(self, docs):\n",
    "        # 문서의 페이지 내용을 이어붙여 반환합니다.\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    def generate_questions(self, text):\n",
    "        \"\"\" 문서 내용에서 질문 생성 \"\"\"\n",
    "        documents_chain = create_stuff_documents_chain(self.config.llm_obj, prompt=self.question_prompt)\n",
    "        question_chain = create_retrieval_chain(None, documents_chain)\n",
    "        result = question_chain.invoke({\"input\": text})\n",
    "        questions = result.split(\"\\n\")  # Assume questions are separated by newlines\n",
    "        return questions\n",
    "\n",
    "    def generate_answers(self, questions, context):\n",
    "        \"\"\" 질문 리스트에 대한 답변 생성 \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # 각 질문에 대해 처리\n",
    "        for question in questions:\n",
    "            print(f\"질문 : {question}\")\n",
    "            answer_chain = create_stuff_documents_chain(self.config.llm_obj, prompt=self.answer_prompt)\n",
    "            rag_chain = create_retrieval_chain(None, answer_chain)\n",
    "            \n",
    "            # 답변 생성\n",
    "            result = rag_chain.invoke({\"input\": question, \"context\": context})\n",
    "            print(f\"답변: {result}\\n================================================\")\n",
    "            \n",
    "            # 결과 저장\n",
    "            results.append({\n",
    "                \"Question\": question,\n",
    "                \"Answer\": result\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459c4cb-3117-41e0-bb4b-4d9061b83395",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 초기 세팅\n",
    "    config = Config(\n",
    "        llm_name=\"upstage/SOLAR-10.7B-Instruct-v1.0\", \n",
    "        embedding_name=\"intfloat/multilingual-e5-large\",\n",
    "        concept=\"financial_analysis\"\n",
    "    )\n",
    "    \n",
    "    # PDF 파일 리스트 정의\n",
    "    df = pd.read_csv(config.train_csv_path)\n",
    "    \n",
    "    # 파이프라인 초기화\n",
    "    dipipeline = DataIngestionPipeline(config)\n",
    "    \n",
    "    # 벡터 데이터베이스 초기화 - 메모리에 저장\n",
    "    pdf_databases = dipipeline.init_vector_db(df)\n",
    "    \n",
    "    # 검색 및 질문 생성\n",
    "    search_engine = FISearch(config, question_prompt=question_prompt, answer_prompt=answer_prompt)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for pdf_title, db_info in pdf_databases.items():\n",
    "        context = search_engine.format_docs(db_info['vector_store'].get_documents())\n",
    "        questions = search_engine.generate_questions(context)\n",
    "        answers = search_engine.generate_answers(questions, context)\n",
    "        \n",
    "        for answer in answers:\n",
    "            all_results.append({\n",
    "                \"Source\": pdf_title,\n",
    "                \"Source_path\": f'./train_source/{pdf_title}',\n",
    "                \"Question\": answer[\"Question\"],\n",
    "                \"Answer\": answer[\"Answer\"]\n",
    "            })\n",
    "    \n",
    "    # 결과를 제출 양식에 맞게 저장\n",
    "    submit_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # 결과를 CSV 파일로 저장\n",
    "    submit_df.to_csv('augmentation_train_data.csv', encoding='UTF-8-sig', index=False)\n",
    "    \n",
    "    print(\"결과가 성공적으로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
